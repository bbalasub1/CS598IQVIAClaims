{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cost_prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balaksuiuc/CS598IQVIAClaims/blob/main/src/cost_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jej5JXayNs"
      },
      "source": [
        "# **CS598 Deep Learning for Healthcare**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7Q5_3OctOD"
      },
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV62d0RgcLoQ"
      },
      "source": [
        "### 1.1 Change the google colab settings\n",
        "We can use a GPU on the google colab by setting below.  \n",
        "**Edit -> Notebook setting -> Hardware accelerator -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEKo8TAeb9TB"
      },
      "source": [
        "### 1.2 Check if the GPU is available in the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uhApS8Qa2yl",
        "outputId": "f309ece9-acd3-4e5c-a4b9-91cde15521d2"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Device name: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device name: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKcEgpY6ckkX"
      },
      "source": [
        "### 1.3 GPU setting for PyTourch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "9aCKdPN0bkYo",
        "outputId": "1cd5a739-380d-486e-ccf9-f4098b7eee27"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import torch\n",
        "\n",
        "# Tell PyTorch to use the GPU\n",
        "device = torch.device(\"cuda\")\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a48846d290a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tell PyTorch to use the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSDxTfE8dKyo"
      },
      "source": [
        "### 1.4 import necessary packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN3iA6mgbyNA"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06BG-pJ84sm"
      },
      "source": [
        "1.5 Set seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guhj1u9k8M1I"
      },
      "source": [
        "# set seed\n",
        "seed = 100\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QktahRUdTgj"
      },
      "source": [
        "## **2. Dataset loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X87pA2We0fb"
      },
      "source": [
        "### 2.1 Load the IQVIA data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqDmVIIhBA9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjt8F4t4kjS5"
      },
      "source": [
        "[Note]\n",
        "Need to upload the iqvia data to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjGFsD2dPjH"
      },
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/iqvia_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqP7CZjfFko"
      },
      "source": [
        "import pandas as pd\n",
        "ENROLL_FILE = DATA_DIR + 'enroll_synth.dat'\n",
        "CLAIMS_2019 = DATA_DIR + 'claims_2019.dat'\n",
        "CLAIMS_2018 = DATA_DIR + 'claims_2018.dat'\n",
        "CLAIMS_2017 = DATA_DIR + 'claims_2017.dat'\n",
        "CLAIMS_2016 = DATA_DIR + 'claims_2016.dat'\n",
        "CLAIMS_2015 = DATA_DIR + 'claims_2015.dat'\n",
        "\n",
        "df_enroll = pd.read_csv(ENROLL_FILE, sep='|', low_memory=False)\n",
        "\n",
        "df_claims2019 = pd.read_csv(CLAIMS_2019, sep='|', low_memory=False)\n",
        "df_claims2018 = pd.read_csv(CLAIMS_2018, sep='|', low_memory=False)\n",
        "df_claims2017 = pd.read_csv(CLAIMS_2017, sep='|', low_memory=False)\n",
        "df_claims2016 = pd.read_csv(CLAIMS_2016, sep='|', low_memory=False)\n",
        "df_claims2015 = pd.read_csv(CLAIMS_2015, sep='|', low_memory=False)\n",
        "\n",
        "## Add year and create a single dataset for claims\n",
        "df_claims2015[\"year\"] = 2015\n",
        "df_claims2016[\"year\"] = 2016\n",
        "df_claims2017[\"year\"] = 2017\n",
        "df_claims2018[\"year\"] = 2018\n",
        "df_claims2019[\"year\"] = 2019\n",
        "\n",
        "list_of_claims = [df_claims2015, df_claims2016, df_claims2017, df_claims2018, df_claims2019]\n",
        "df_claims = pd.concat(list_of_claims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlzdYrkHncNq"
      },
      "source": [
        "# enroll data\n",
        "print(\"Shape of Claims{}\".format(df_enroll.shape))\n",
        "df_enroll.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgR9yr9anOQd"
      },
      "source": [
        "# claim data\n",
        "print(\"Shape of Claims{}\".format(df_claims.shape))\n",
        "df_claims.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqr7SvhlMMw"
      },
      "source": [
        "### 2.2 Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cCE5CJtfl96"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of patients across regions\n",
        "rd = df_enroll[\"pat_region\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients across regions\")\n",
        "\n",
        "# Distribution of patients gender\n",
        "rd = df_enroll[\"der_sex\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients' gender\")\n",
        "\n",
        "# Distribution of Age\n",
        "df_enroll[\"age\"] = 2021 - df_enroll[\"der_yob\"]\n",
        "\n",
        "rd = df_enroll[df_enroll[\"der_yob\"] > 1900][\"age\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of patients' age\")\n",
        "\n",
        "# Get the count of claims paid (and denied)\n",
        "df_claims[\"pmt_st_cd\"].value_counts()\n",
        "\n",
        "# number of diagnosis populated in each claim\n",
        "diag_cols = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \"diag12\"]\n",
        "df_claims[\"num_of_diag\"] = df_claims[diag_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_diag\"].mean()\n",
        "\n",
        "# number of icdprc populated in each claim\n",
        "icdprc_cols=[\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\", \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "df_claims[\"num_of_icdprc\"] = df_claims[icdprc_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_icdprc\"].mean()\n",
        "\n",
        "diag = []\n",
        "for colname in diag_cols:\n",
        "    diag.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(diag)))\n",
        "# 22138\n",
        "\n",
        "prc = []\n",
        "for colname in icdprc_cols:\n",
        "    prc.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(prc)))\n",
        "# 926\n",
        "\n",
        "\n",
        "# number of claims with same day service\n",
        "sum(df_claims[\"from_dt\"] == df_claims[\"to_dt\"])\n",
        "# 2378556 out of 2438054 i.e. 97.5%\n",
        "\n",
        "# Distribution of charges\n",
        "rd = df_claims[\"charge\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution charges\")\n",
        "\n",
        "# Log charges makes more sense\n",
        "# filtering out rows where charges are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"charge\"] > 1][\"charge\"]).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of Log of Charges\")\n",
        "\n",
        "# Distribution of Paid amounts\n",
        "# filtering out rows where paid are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"paid\"] > 1][\"paid\"]).plot(kind='hist', bins=25)\n",
        "rd.set_title(\"Distribution of Log of Paid\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Checking the unique number of patients in the datasets\n",
        "print(len(pd.unique(df_enroll['pat_id'])))\n",
        "# 30000\n",
        "print(len(pd.unique(df_claims2015['pat_id'])))\n",
        "# 18927\n",
        "print(len(pd.unique(df_claims2016['pat_id'])))\n",
        "#21483\n",
        "print(len(pd.unique(df_claims2017['pat_id'])))\n",
        "#15190\n",
        "print(len(pd.unique(df_claims2018['pat_id'])))\n",
        "#6445\n",
        "print(len(pd.unique(df_claims2019['pat_id'])))\n",
        "#4884"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNc02_fxghT"
      },
      "source": [
        "## **3. Data embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmy3BPKB0Nss"
      },
      "source": [
        "### 3.1 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs1T_C4ixruV"
      },
      "source": [
        "# Wei's code here\n",
        "#assign quarter to each record and convert 'to_dt' to datetime format\n",
        "df_claims[\"quarter\"] = pd.PeriodIndex(pd.to_datetime(df_claims[\"to_dt\"]), freq = 'Q')\n",
        "df_claims['to_dt'] = pd.to_datetime(df_claims['to_dt'])\n",
        "\n",
        "#find the length of claims for each patient\n",
        "df_claims_length = df_claims[['pat_id','to_dt']].groupby('pat_id').agg({'to_dt':['max','min']}).reset_index() \n",
        "df_claims_length.columns = ['pat_id','max','min']\n",
        "df_claims_length['length'] = df_claims_length['max'] - df_claims_length['min']\n",
        "df_claims_length['length'] = df_claims_length['length'].dt.days\n",
        "avg_claim_length = df_claims_length['length'].mean()\n",
        "min_claim_length = df_claims_length['length'].min()\n",
        "max_claim_length = df_claims_length['length'].max()\n",
        "print(min_claim_length,avg_claim_length,max_claim_length)\n",
        "rd = df_claims_length['length'].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Frequency of Length\")\n",
        "#most patients's length is less than three years, 12 quarters, so chooce an observation window of 12 quarters\n",
        "\n",
        "#calculate index date = last claim - 270 days (180days as prediction window + 90days as last quarter)\n",
        "claim_indx_date = df_claims[['pat_id','to_dt']].groupby('pat_id').agg({'to_dt':['max']}).reset_index()\n",
        "claim_indx_date.columns = ['pat_id','max']\n",
        "claim_indx_date['indx_date'] = claim_indx_date['max'] - pd.to_timedelta(270,unit='d')\n",
        "#filter claims, \n",
        "filterred_claims = pd.merge(df_claims,claim_indx_date,how = 'left',on=['pat_id'])\n",
        "#observation includes 1000 days(three years) before index date\n",
        "#prediction window is 180 days (two quarters), last 90 days(last quarter) is reserved as target\n",
        "filterred_observation = filterred_claims.loc[filterred_claims.to_dt<=filterred_claims.indx_date]\n",
        "filterred_observation = filterred_observation.loc[filterred_observation.to_dt>=filterred_observation.indx_date-pd.to_timedelta(1095,unit='d')]\n",
        "filterred_observation = filterred_observation.loc[filterred_observation.to_dt>=pd.to_datetime('2015-10-1')] #code changed after October 1, 2015\n",
        "filterred_target = filterred_claims.loc[filterred_claims.to_dt>=filterred_claims.indx_date+pd.to_timedelta(180,unit='d')]\n",
        "\n",
        "#find the number of claims after filter\n",
        "pat_claimsnumb = filterred_observation[['pat_id']].groupby('pat_id').agg({'pat_id':['count']}).reset_index()\n",
        "pat_claimsnumb.columns = ['pat_id','count']\n",
        "print(pat_claimsnumb.shape,pat_claimsnumb['count'].max(),pat_claimsnumb['count'].mean(),pat_claimsnumb['count'].min())\n",
        "rd = np.log10(pat_claimsnumb['count']).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of claim numbers in log 10\")\n",
        "\n",
        "iag_cols = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \"diag12\"]\n",
        "icdprc_cols=[\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\", \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "\n",
        "#number of unique diag codes and prc codes after filter\n",
        "diag = []\n",
        "for colname in diag_cols:\n",
        "    diag.extend(pd.unique(filterred_observation[colname]))\n",
        "diag_dict = np.unique(diag)\n",
        "print(len(np.unique(diag)))\n",
        "\n",
        "prc = []\n",
        "for colname in icdprc_cols:\n",
        "    prc.extend(pd.unique(filterred_observation[colname]))\n",
        "prc_dict = np.unique(prc)\n",
        "print(len(np.unique(prc)))\n",
        "\n",
        "#number of unique record type, procedure code and revenue code(high-level description of services)\n",
        "filterred_observation[\"rectype\"] = filterred_observation[\"rectype\"].astype('str')\n",
        "filterred_observation[\"proc_cde\"] = filterred_observation[\"proc_cde\"].astype('str')\n",
        "filterred_observation[\"rev_code\"] = filterred_observation[\"rev_code\"].astype('str')\n",
        "print(len(np.unique(filterred_observation[\"rectype\"])))\n",
        "print(len(np.unique(filterred_observation[\"proc_cde\"])))\n",
        "print(len(np.unique(filterred_observation[\"rev_code\"])))\n",
        "rectype_dict = np.unique(filterred_observation[\"rectype\"])\n",
        "proc_cde_dict = np.unique(filterred_observation[\"proc_cde\"])\n",
        "rev_code_dict = np.unique(filterred_observation[\"rev_code\"])\n",
        "\n",
        "#extract interested columns\n",
        "filtered_features = filterred_observation[[\"pat_id\",\"paid\",\"charge\",\"quarter\",\"rectype\",\"proc_cde\",\"rev_code\",\"diag1\", \"diag2\", \"diag3\", \n",
        "                                           \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\",\"diag12\",\"icdprc1\", \n",
        "                                           \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\",\"icdprc10\",\n",
        "                                           \"icdprc11\", \"icdprc12\"]]\n",
        "\n",
        "print(filtered_features.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzhtkICX13WX"
      },
      "source": [
        "#investigate unqiue codes after filter\n",
        "proc_cde_valuecount = filtered_features[\"proc_cde\"].value_counts()\n",
        "rev_code_valuecount = filtered_features[\"rev_code\"].value_counts()\n",
        "diag_valuecount = filtered_features[\"diag1\"].value_counts()\n",
        "prc_valuecount = filtered_features[\"icdprc1\"].value_counts()\n",
        "\n",
        "proc_cde_valuecount = proc_cde_valuecount.to_frame()\n",
        "rd = np.log10(proc_cde_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of proc_cde\")\n",
        "#among 6114, less than 500 proc codes appear more than 100 times\n",
        "rev_code_valuecount = rev_code_valuecount.to_frame()\n",
        "rd = np.log10(rev_code_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of revenue code\")\n",
        "#among 718, less than 300 revenue codes appear more than 100 times\n",
        "diag_valuecount = diag_valuecount.to_frame()\n",
        "rd = np.log10(diag_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of diag code\")\n",
        "#among 12952, about 1000 diag codes appear more than 100 times\n",
        "prc_valuecount = prc_valuecount.to_frame()\n",
        "rd = np.log10(prc_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of prc code\")\n",
        "#among 420, less than 50 appear more than 100 times\n",
        "\n",
        "proc_cde_valuecount = filtered_features[\"proc_cde\"].value_counts().reset_index()\n",
        "rev_code_valuecount = filtered_features[\"rev_code\"].value_counts().reset_index()\n",
        "diag_valuecount = filtered_features[\"diag1\"].value_counts().reset_index()\n",
        "prc_valuecount = filtered_features[\"icdprc1\"].value_counts().reset_index()\n",
        "\n",
        "#apply thresholds on codes to extract dictionary for high frequency codes\n",
        "proc_cde_dict = set(proc_cde_valuecount.apply(lambda x: x['index'] if x['proc_cde']>=1000 else 0,axis=1).to_list())\n",
        "rev_code_dict = set(rev_code_valuecount.apply(lambda x: x['index'] if x['rev_code']>=1000 else 0,axis=1).to_list())\n",
        "diag_dict = set(diag_valuecount.apply(lambda x: x['index'] if x['diag1']>=1000 else 0,axis=1).to_list())\n",
        "prc_dict = set(prc_valuecount.apply(lambda x: x['index'] if x['icdprc1']>=100 else 0,axis=1).to_list())\n",
        "proc_cde_dict.remove(0)\n",
        "rev_code_dict.remove(0)\n",
        "diag_dict.remove(0)\n",
        "prc_dict.remove(0)\n",
        "\n",
        "print(len(proc_cde_dict),len(rev_code_dict),len(diag_dict),len(prc_dict))\n",
        "#total code features are reduced to less than 300 after filtering\n",
        "\n",
        "#filter out code features that not appears in the dictionary\n",
        "def combine_columns_into_list(x,cols,dict_set):\n",
        "    out = list()\n",
        "    for col in cols:\n",
        "        if x[col] in dict_set:\n",
        "            out.append(x[col])\n",
        "    return out\n",
        "\n",
        "filtered_features['diags'] = filtered_features.apply(lambda x: combine_columns_into_list(x,[\"diag1\",\"diag2\",\"diag3\",\"diag4\",\"diag5\",\"diag6\",\"diag7\",\"diag8\",\"diag9\",\"diag10\"],diag_dict),axis=1)\n",
        "filtered_features['rev_code'] = filtered_features.apply(lambda x: x['rev_code'] if x['rev_code'] in rev_code_dict else 0,axis=1)\n",
        "filtered_features['icdprc'] = filtered_features.apply(lambda x: x['icdprc1'] if x['icdprc1'] in prc_dict else 0,axis=1)\n",
        "filtered_features['proc_cde'] = filtered_features.apply(lambda x: x['proc_cde'] if x['proc_cde'] in proc_cde_dict else 0,axis=1)\n",
        "\n",
        "#drop original code columns\n",
        "dropped_features = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \n",
        "                \"diag12\",\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\",\n",
        "                \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "filtered_features = filtered_features.drop(dropped_features,axis=1)\n",
        "\n",
        "print(filtered_features.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URl6Z-bJ20d6"
      },
      "source": [
        "#build multi-hot for rectype, proc_cde, rev_code, recvtype and icdprc\n",
        "filtered_features = pd.get_dummies(filtered_features,columns = ['rectype','proc_cde','rev_code','icdprc'])\n",
        "print(filtered_features.shape)\n",
        "filtered_dict_features = filtered_features.copy(deep=True)\n",
        "\n",
        "#build multi-hot for diags list feature\n",
        "v = filtered_dict_features.diags.values\n",
        "l = [len(x) for x in v.tolist()]\n",
        "f, u = pd.factorize(np.concatenate(v))\n",
        "n, m = len(v), u.size\n",
        "i = np.arange(n).repeat(l)\n",
        "\n",
        "dummies = pd.DataFrame(\n",
        "    np.bincount(i * m + f, minlength=n * m).reshape(n, m),\n",
        "    filtered_dict_features.index, u\n",
        ")\n",
        "\n",
        "filtered_dict_features = filtered_dict_features.drop('diags', 1).join(dummies)\n",
        "\n",
        "#filtered_dict_features.to_pickle('./filterd_feature_dict_281.pkl')\n",
        "filtered_dict_features = filtered_dict_features.groupby(['pat_id','quarter']).sum().reset_index()\n",
        "#sort the pat_id and quarter in ascending order\n",
        "filtered_dict_features = filtered_dict_features.sort_values(by=['pat_id','quarter'],ascending=False)\n",
        "\n",
        "#maximum quarters of all patients, will truncate to 12\n",
        "maxnum_quarters = 0\n",
        "for pat in pat_ids_dict:\n",
        "    x = filtered_dict_features.loc[filtered_dict_features['pat_id']==pat]\n",
        "    maxnum_quarters = max(maxnum_quarters,x.shape[0])\n",
        "print(maxnum_quarters) #13\n",
        "\n",
        "#construct data for model training\n",
        "x = np.zeros((18474,279*12))\n",
        "#build pat_id list to extract data from filtered_dict_features\n",
        "pat_ids_dict = set(np.unique(filtered_dict_features['pat_id']))\n",
        "pat_ids_dict = sorted(list(pat_ids_dict),reverse=True)\n",
        "\n",
        "## construct x matrix\n",
        "num_pat = 0\n",
        "for pat in pat_ids_dict:\n",
        "    pat_val = filtered_dict_features.loc[filtered_dict_features['pat_id']==pat]\n",
        "    pat_val = pat_val.drop(['pat_id','quarter'],axis=1)\n",
        "    raw_values = pat_val.values.tolist()\n",
        "    if len(raw_values)>12:\n",
        "        raw_values = raw_values[:-1]\n",
        "    values = [item for sublist in raw_values for item in sublist]\n",
        "    x[num_pat,:len(values)] = values\n",
        "    num_pat+=1\n",
        "\n",
        "#np.savetxt(\"x_18474_3348.csv\", x, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vICrkdVa3z5T"
      },
      "source": [
        "#build the y vector\n",
        "y_paid = filterred_target[['pat_id','paid']].groupby('pat_id').agg({'paid':['sum']}).reset_index() \n",
        "y_charge = filterred_target[['pat_id','charge']].groupby('pat_id').agg({'charge':['sum']}).reset_index() \n",
        "y_paid.columns = ['pat_id','paid']\n",
        "y_charge.columns = ['pat_id','charge']\n",
        "print(y_paid.shape,y_charge.shape)\n",
        "print(y_paid.isnull().values.any(),y_charge.isnull().values.any())\n",
        "\n",
        "y_charge['miss'] = y_charge.apply(lambda x: 0 if x['pat_id'] in pat_ids_dict else 1,axis=1)\n",
        "y_charge = y_charge.loc[y_charge['miss']!=1]\n",
        "y_charge = y_charge.drop(['miss'],axis=1)\n",
        "y_charge = y_charge.sort_values(by=['pat_id'],ascending = False)\n",
        "\n",
        "y_paid['miss'] = y_paid.apply(lambda x: 0 if x['pat_id'] in pat_ids_dict else 1,axis=1)\n",
        "y_paid = y_paid.loc[y_paid['miss']!=1]\n",
        "y_paid = y_paid.drop(['miss'],axis=1)\n",
        "y_paid = y_paid.sort_values(by=['pat_id'],ascending = False)\n",
        "\n",
        "#patient id is matched row by row between y_paid and x\n",
        "print(y_charge['pat_id'].to_list()==pat_ids_dict)\n",
        "print(y_paid['pat_id'].to_list()==pat_ids_dict)\n",
        "#True True\n",
        "\n",
        "ypaid = np.array(y_paid['paid'].to_list())\n",
        "#np.savetxt(\"ypaid.csv\", ypaid, delimiter=\",\")\n",
        "ycharge = np.array(y_charge['charge'].to_list())\n",
        "#np.savetxt(\"ycharge.csv\", ycharge, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSCphZS5pll"
      },
      "source": [
        "# Load csv data that is already preprocessed by Wei\n",
        "# -> Wei's code should be in this ipynb?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCk2xBeR61c"
      },
      "source": [
        "#\n",
        "def read_emb_csv(filepath):\n",
        "    pd_x = pd.read_csv(filepath + 'x_18474_3348.csv')\n",
        "    pd_y_charge = pd.read_csv(filepath + 'ycharge.csv')\n",
        "    pd_y_paid = pd.read_csv(filepath + 'ypaid.csv')\n",
        "\n",
        "    return pd_x, pd_y_charge, pd_y_paid\n",
        "\n",
        "# load the emb csv\n",
        "EMB_DATA_PATH = '/content/drive/MyDrive/iqvia_data/'\n",
        "pd_x, pd_y_charge, pd_y_paid = read_emb_csv(EMB_DATA_PATH) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3cDoh8DhZKb"
      },
      "source": [
        "print(\"pd_x: {}\".format(pd_x.shape))\n",
        "print(\"pd_y_charge: {}\".format(pd_y_charge.shape))\n",
        "print(\"ppd_y_paid: {}\".format(pd_y_paid.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLGZrWoxnC-"
      },
      "source": [
        "# Input data normalization\n",
        "# https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%AD%A3%E8%A6%8F%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86/#toc4\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        " \n",
        "scaler_mm = MinMaxScaler()\n",
        "scaler_mm.fit(pd_x)\n",
        "pd_x = pd.DataFrame(scaler_mm.transform(pd_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOhYhvn1nOf5"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        if 1:\n",
        "            # convert to torch.tensor\n",
        "            self.x = torch.tensor(x.values.astype(np.float32))\n",
        "            self.y = torch.tensor(y.values.astype(np.float32))\n",
        "        else:\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #return self.x.loc[index][:], self.y.loc[index]\n",
        "        return self.x[index][:], self.y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28Z3vjUny3W"
      },
      "source": [
        "dataset = CustomDataset(pd_x, pd_y_paid)\n",
        "print(len(dataset))\n",
        "#print(dataset[0][0])\n",
        "#print(dataset[0][0].shape)\n",
        "#print(dataset[0][1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apglZFjRkFfH"
      },
      "source": [
        "### 3.2 Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ewGGH8mH7g"
      },
      "source": [
        "# https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_len = int(len(dataset)*0.7)\n",
        "val_len = int(len(dataset)*0.2)\n",
        "lengths = [train_len, val_len, len(dataset) - train_len - val_len]\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"train data length: {}\".format(len(train_dataset)))\n",
        "print(\"val data length: {}\".format(len(val_dataset)))\n",
        "print(\"test data length: {}\".format(len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAf9Ps830gPc"
      },
      "source": [
        "### 3.3 Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMMgsNo6SNb"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(12931/32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_YpIWHlOdgf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik5rGVmiOSIv"
      },
      "source": [
        "# Test\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "x, y = next(train_iter)\n",
        "\n",
        "print('Shape of a batch x:', x.shape)\n",
        "print('Shape of a batch y:', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gOXYHqyqPI"
      },
      "source": [
        "## **4. Model building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnqKGxz_9ku-"
      },
      "source": [
        "### **4.1 Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yocN3DEg93O7"
      },
      "source": [
        "# Bala, Kautuk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJsYbtTZ8QAt"
      },
      "source": [
        "### **4.2 Base model(Drewe-Bossâ€™s paper)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPcItHq0ys_G"
      },
      "source": [
        "# The purpose of this model is to reproduce the prior model proposed by Drewe-Boss et al.  Deep learning for prediction of population health costs\n",
        "\n",
        "# This code is inspired by \n",
        "#  (1) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "#  (2) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/VNfPA/homework-4-mina/lab\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Basemodel(nn.Module):\n",
        "    def __init__(self, input_size=3348, output_size=1):\n",
        "        super(Basemodel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = 50\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(in_features=self.hidden_size+self.input_size, out_features=self.output_size)\n",
        "        self.do = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x = self.do(F.relu(self.fc1(x)))\n",
        "        x = self.do(F.relu(self.fc2(x)))\n",
        "        x = self.do(F.relu(self.fc3(x)))\n",
        "        #print(x.shape)\n",
        "        #print(x0.shape)\n",
        "        x = torch.cat((x, x0), dim=1)\n",
        "        #print(x.shape)\n",
        "        if 0:\n",
        "            x = self.do(F.relu(self.fc4(x)))\n",
        "        else:\n",
        "            #x = self.fc4(x)\n",
        "            x = F.relu(self.fc4(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Basemodel() # Need to specify the input/output size(we have not decided input and output data)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz72uokf-LVQ"
      },
      "source": [
        "### **4.3 Advanced model(Base model + alpha)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr8bwEYL_E15"
      },
      "source": [
        "# Idea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upr9qp1AOpTV"
      },
      "source": [
        "## 5. **Model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvR5NLUEO0KQ"
      },
      "source": [
        "### 5.1 Loss and potimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N0LMlIOyB4"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Learning rate is not written in the prior papar. => Experimental value\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5utr43ZHPLTd"
      },
      "source": [
        "### 5.2 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZLUpgwPRH1"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "\n",
        "from sklearn.metrics import *\n",
        "\n",
        "def regression_metrics(Y_pred, Y_True):\n",
        "    # Evaluation of methods: \n",
        "    # 1. Pearson's correlation (r), \n",
        "    # 2. Spearman's correlation (\u001a),\n",
        "    # 3. Mean absolute prediction error (MAPE),\n",
        "    # 4. R squared (r2),\n",
        "    # 5. Cumming's Prediction Measure (CPM)\n",
        "    mape, r2 = mean_absolute_error(Y_True, Y_pred), \\\n",
        "                r2_score(Y_True, Y_pred)\n",
        "\n",
        "    return mape, r2\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    all_y_true = torch.LongTensor()\n",
        "    all_y_pred = torch.LongTensor()\n",
        "\n",
        "    val_loss = 0\n",
        "    for x, y in val_loader:\n",
        "        y_pred = model(x)\n",
        "        \n",
        "        all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
        "        all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)\n",
        "        loss = criterion(y_pred, y)\n",
        "        val_loss += loss.item()\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    mape, r2 = regression_metrics(all_y_pred, all_y_true)\n",
        "    #print(f\"mape: {mape:.3f}, r2: {r2:.3f}\")\n",
        "    return val_loss, mape, r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ed2StcLIMN"
      },
      "source": [
        "# test without training\n",
        "evaluate(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9M-byiPORg"
      },
      "source": [
        "### 5.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4S1IXYPQV3"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss= 0\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            #print(y_pred.shape)\n",
        "            if 1:\n",
        "                # convert shape from [batch size, 1] to [batch size]\n",
        "                y_pred = y_pred.view(y_pred.shape[0])\n",
        "                y = y.view(y.shape[0])\n",
        "                #print(y_pred.shape)\n",
        "                #print(y.shape)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        val_loss, mape, r2 = evaluate(model, val_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f} \\t validation Loss: {:.6f}'.format(epoch+1, train_loss, val_loss))\n",
        "        print('Epoch: %d \\t Validation mape: %.2f, r2: %.2f'%(epoch+1, mape, r2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HwbAOCSZPM1"
      },
      "source": [
        "n_epochs = 10000 # the prior papar's n_epochs=25\n",
        "train(model, train_loader, val_loader, n_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU1JuOCGcIvN"
      },
      "source": [
        "### 6. **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjQhOCG2Zk1L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
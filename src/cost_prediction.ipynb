{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cost_prediction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jej5JXayNs"
      },
      "source": [
        "# **CS598 Deep Learning for Healthcare**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7Q5_3OctOD"
      },
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV62d0RgcLoQ"
      },
      "source": [
        "### 1.1 Change the google colab settings\n",
        "We can use a GPU on the google colab by setting below.  \n",
        "**Edit -> Notebook setting -> Hardware accelerator -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEKo8TAeb9TB"
      },
      "source": [
        "### 1.2 Check if the GPU is available in the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uhApS8Qa2yl",
        "outputId": "f309ece9-acd3-4e5c-a4b9-91cde15521d2"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Device name: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device name: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKcEgpY6ckkX"
      },
      "source": [
        "### 1.3 GPU setting for PyTourch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "9aCKdPN0bkYo",
        "outputId": "1cd5a739-380d-486e-ccf9-f4098b7eee27"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import torch\n",
        "\n",
        "# Tell PyTorch to use the GPU\n",
        "device = torch.device(\"cuda\")\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a48846d290a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tell PyTorch to use the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSDxTfE8dKyo"
      },
      "source": [
        "### 1.4 import necessary packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN3iA6mgbyNA"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06BG-pJ84sm"
      },
      "source": [
        "1.5 Set seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guhj1u9k8M1I"
      },
      "source": [
        "# set seed\n",
        "seed = 100\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QktahRUdTgj"
      },
      "source": [
        "## **2. Dataset loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X87pA2We0fb"
      },
      "source": [
        "### 2.1 Load the IQVIA data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqDmVIIhBA9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjt8F4t4kjS5"
      },
      "source": [
        "[Note]\n",
        "Need to upload the iqvia data to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjGFsD2dPjH"
      },
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/iqvia_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqP7CZjfFko"
      },
      "source": [
        "import pandas as pd\n",
        "ENROLL_FILE = DATA_DIR + 'enroll_synth.dat'\n",
        "CLAIMS_2019 = DATA_DIR + 'claims_2019.dat'\n",
        "CLAIMS_2018 = DATA_DIR + 'claims_2018.dat'\n",
        "CLAIMS_2017 = DATA_DIR + 'claims_2017.dat'\n",
        "CLAIMS_2016 = DATA_DIR + 'claims_2016.dat'\n",
        "CLAIMS_2015 = DATA_DIR + 'claims_2015.dat'\n",
        "\n",
        "df_enroll = pd.read_csv(ENROLL_FILE, sep='|', low_memory=False)\n",
        "\n",
        "df_claims2019 = pd.read_csv(CLAIMS_2019, sep='|', low_memory=False)\n",
        "df_claims2018 = pd.read_csv(CLAIMS_2018, sep='|', low_memory=False)\n",
        "df_claims2017 = pd.read_csv(CLAIMS_2017, sep='|', low_memory=False)\n",
        "df_claims2016 = pd.read_csv(CLAIMS_2016, sep='|', low_memory=False)\n",
        "df_claims2015 = pd.read_csv(CLAIMS_2015, sep='|', low_memory=False)\n",
        "\n",
        "## Add year and create a single dataset for claims\n",
        "df_claims2015[\"year\"] = 2015\n",
        "df_claims2016[\"year\"] = 2016\n",
        "df_claims2017[\"year\"] = 2017\n",
        "df_claims2018[\"year\"] = 2018\n",
        "df_claims2019[\"year\"] = 2019\n",
        "\n",
        "list_of_claims = [df_claims2015, df_claims2016, df_claims2017, df_claims2018, df_claims2019]\n",
        "df_claims = pd.concat(list_of_claims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlzdYrkHncNq"
      },
      "source": [
        "# enroll data\n",
        "print(\"Shape of Claims{}\".format(df_enroll.shape))\n",
        "df_enroll.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgR9yr9anOQd"
      },
      "source": [
        "# claim data\n",
        "print(\"Shape of Claims{}\".format(df_claims.shape))\n",
        "df_claims.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqr7SvhlMMw"
      },
      "source": [
        "### 2.2 Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cCE5CJtfl96"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of patients across regions\n",
        "rd = df_enroll[\"pat_region\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients across regions\")\n",
        "\n",
        "# Distribution of patients gender\n",
        "rd = df_enroll[\"der_sex\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients' gender\")\n",
        "\n",
        "# Distribution of Age\n",
        "df_enroll[\"age\"] = 2021 - df_enroll[\"der_yob\"]\n",
        "\n",
        "rd = df_enroll[df_enroll[\"der_yob\"] > 1900][\"age\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of patients' age\")\n",
        "\n",
        "# Get the count of claims paid (and denied)\n",
        "df_claims[\"pmt_st_cd\"].value_counts()\n",
        "\n",
        "# number of diagnosis populated in each claim\n",
        "diag_cols = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \"diag12\"]\n",
        "df_claims[\"num_of_diag\"] = df_claims[diag_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_diag\"].mean()\n",
        "\n",
        "# number of icdprc populated in each claim\n",
        "icdprc_cols=[\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\", \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "df_claims[\"num_of_icdprc\"] = df_claims[icdprc_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_icdprc\"].mean()\n",
        "\n",
        "diag = []\n",
        "for colname in diag_cols:\n",
        "    diag.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(diag)))\n",
        "# 22138\n",
        "\n",
        "prc = []\n",
        "for colname in icdprc_cols:\n",
        "    prc.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(prc)))\n",
        "# 926\n",
        "\n",
        "\n",
        "# number of claims with same day service\n",
        "sum(df_claims[\"from_dt\"] == df_claims[\"to_dt\"])\n",
        "# 2378556 out of 2438054 i.e. 97.5%\n",
        "\n",
        "# Distribution of charges\n",
        "rd = df_claims[\"charge\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution charges\")\n",
        "\n",
        "# Log charges makes more sense\n",
        "# filtering out rows where charges are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"charge\"] > 1][\"charge\"]).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of Log of Charges\")\n",
        "\n",
        "# Distribution of Paid amounts\n",
        "# filtering out rows where paid are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"paid\"] > 1][\"paid\"]).plot(kind='hist', bins=25)\n",
        "rd.set_title(\"Distribution of Log of Paid\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Checking the unique number of patients in the datasets\n",
        "print(len(pd.unique(df_enroll['pat_id'])))\n",
        "# 30000\n",
        "print(len(pd.unique(df_claims2015['pat_id'])))\n",
        "# 18927\n",
        "print(len(pd.unique(df_claims2016['pat_id'])))\n",
        "#21483\n",
        "print(len(pd.unique(df_claims2017['pat_id'])))\n",
        "#15190\n",
        "print(len(pd.unique(df_claims2018['pat_id'])))\n",
        "#6445\n",
        "print(len(pd.unique(df_claims2019['pat_id'])))\n",
        "#4884"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNc02_fxghT"
      },
      "source": [
        "## **3. Data embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmy3BPKB0Nss"
      },
      "source": [
        "### 3.1 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs1T_C4ixruV"
      },
      "source": [
        "# Wei's code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSCphZS5pll"
      },
      "source": [
        "# Load csv data that is already preprocessed by Wei\n",
        "# -> Wei's code should be in this ipynb?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCk2xBeR61c"
      },
      "source": [
        "#\n",
        "def read_emb_csv(filepath):\n",
        "    pd_x = pd.read_csv(filepath + 'x_18474_3348.csv')\n",
        "    pd_y_charge = pd.read_csv(filepath + 'ycharge.csv')\n",
        "    pd_y_paid = pd.read_csv(filepath + 'ypaid.csv')\n",
        "\n",
        "    return pd_x, pd_y_charge, pd_y_paid\n",
        "\n",
        "# load the emb csv\n",
        "EMB_DATA_PATH = '/content/drive/MyDrive/iqvia_data/'\n",
        "pd_x, pd_y_charge, pd_y_paid = read_emb_csv(EMB_DATA_PATH) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3cDoh8DhZKb"
      },
      "source": [
        "print(\"pd_x: {}\".format(pd_x.shape))\n",
        "print(\"pd_y_charge: {}\".format(pd_y_charge.shape))\n",
        "print(\"ppd_y_paid: {}\".format(pd_y_paid.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLGZrWoxnC-"
      },
      "source": [
        "# Input data normalization\n",
        "# https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%AD%A3%E8%A6%8F%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86/#toc4\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        " \n",
        "scaler_mm = MinMaxScaler()\n",
        "scaler_mm.fit(pd_x)\n",
        "pd_x = pd.DataFrame(scaler_mm.transform(pd_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOhYhvn1nOf5"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        if 1:\n",
        "            # convert to torch.tensor\n",
        "            self.x = torch.tensor(x.values.astype(np.float32))\n",
        "            self.y = torch.tensor(y.values.astype(np.float32))\n",
        "        else:\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #return self.x.loc[index][:], self.y.loc[index]\n",
        "        return self.x[index][:], self.y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28Z3vjUny3W"
      },
      "source": [
        "dataset = CustomDataset(pd_x, pd_y_paid)\n",
        "print(len(dataset))\n",
        "#print(dataset[0][0])\n",
        "#print(dataset[0][0].shape)\n",
        "#print(dataset[0][1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apglZFjRkFfH"
      },
      "source": [
        "### 3.2 Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ewGGH8mH7g"
      },
      "source": [
        "# https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_len = int(len(dataset)*0.7)\n",
        "val_len = int(len(dataset)*0.2)\n",
        "lengths = [train_len, val_len, len(dataset) - train_len - val_len]\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"train data length: {}\".format(len(train_dataset)))\n",
        "print(\"val data length: {}\".format(len(val_dataset)))\n",
        "print(\"test data length: {}\".format(len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAf9Ps830gPc"
      },
      "source": [
        "### 3.3 Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMMgsNo6SNb"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(12931/32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_YpIWHlOdgf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik5rGVmiOSIv"
      },
      "source": [
        "# Test\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "x, y = next(train_iter)\n",
        "\n",
        "print('Shape of a batch x:', x.shape)\n",
        "print('Shape of a batch y:', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gOXYHqyqPI"
      },
      "source": [
        "## **4. Model building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnqKGxz_9ku-"
      },
      "source": [
        "### **4.1 Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yocN3DEg93O7"
      },
      "source": [
        "# Bala, Kautuk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJsYbtTZ8QAt"
      },
      "source": [
        "### **4.2 Base model(Drewe-Bossâ€™s paper)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPcItHq0ys_G"
      },
      "source": [
        "# The purpose of this model is to reproduce the prior model proposed by Drewe-Boss et al.  Deep learning for prediction of population health costs\n",
        "\n",
        "# This code is inspired by \n",
        "#  (1) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "#  (2) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/VNfPA/homework-4-mina/lab\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Basemodel(nn.Module):\n",
        "    def __init__(self, input_size=3348, output_size=1):\n",
        "        super(Basemodel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = 50\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(in_features=self.hidden_size+self.input_size, out_features=self.output_size)\n",
        "        self.do = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x = self.do(F.relu(self.fc1(x)))\n",
        "        x = self.do(F.relu(self.fc2(x)))\n",
        "        x = self.do(F.relu(self.fc3(x)))\n",
        "        #print(x.shape)\n",
        "        #print(x0.shape)\n",
        "        x = torch.cat((x, x0), dim=1)\n",
        "        #print(x.shape)\n",
        "        if 0:\n",
        "            x = self.do(F.relu(self.fc4(x)))\n",
        "        else:\n",
        "            #x = self.fc4(x)\n",
        "            x = F.relu(self.fc4(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Basemodel() # Need to specify the input/output size(we have not decided input and output data)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz72uokf-LVQ"
      },
      "source": [
        "### **4.3 Advanced model(Base model + alpha)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr8bwEYL_E15"
      },
      "source": [
        "# Idea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upr9qp1AOpTV"
      },
      "source": [
        "## 5. **Model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvR5NLUEO0KQ"
      },
      "source": [
        "### 5.1 Loss and potimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N0LMlIOyB4"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Learning rate is not written in the prior papar. => Experimental value\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5utr43ZHPLTd"
      },
      "source": [
        "### 5.2 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZLUpgwPRH1"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "\n",
        "from sklearn.metrics import *\n",
        "\n",
        "def regression_metrics(Y_pred, Y_True):\n",
        "    # Evaluation of methods: \n",
        "    # 1. Pearson's correlation (r), \n",
        "    # 2. Spearman's correlation (\u001a),\n",
        "    # 3. Mean absolute prediction error (MAPE),\n",
        "    # 4. R squared (r2),\n",
        "    # 5. Cumming's Prediction Measure (CPM)\n",
        "    mape, r2 = mean_absolute_error(Y_True, Y_pred), \\\n",
        "                r2_score(Y_True, Y_pred)\n",
        "\n",
        "    return mape, r2\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    all_y_true = torch.LongTensor()\n",
        "    all_y_pred = torch.LongTensor()\n",
        "\n",
        "    val_loss = 0\n",
        "    for x, y in val_loader:\n",
        "        y_pred = model(x)\n",
        "        \n",
        "        all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
        "        all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)\n",
        "        loss = criterion(y_pred, y)\n",
        "        val_loss += loss.item()\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    mape, r2 = regression_metrics(all_y_pred, all_y_true)\n",
        "    #print(f\"mape: {mape:.3f}, r2: {r2:.3f}\")\n",
        "    return val_loss, mape, r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ed2StcLIMN"
      },
      "source": [
        "# test without training\n",
        "evaluate(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9M-byiPORg"
      },
      "source": [
        "### 5.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4S1IXYPQV3"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss= 0\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            #print(y_pred.shape)\n",
        "            if 1:\n",
        "                # convert shape from [batch size, 1] to [batch size]\n",
        "                y_pred = y_pred.view(y_pred.shape[0])\n",
        "                y = y.view(y.shape[0])\n",
        "                #print(y_pred.shape)\n",
        "                #print(y.shape)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        val_loss, mape, r2 = evaluate(model, val_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f} \\t validation Loss: {:.6f}'.format(epoch+1, train_loss, val_loss))\n",
        "        print('Epoch: %d \\t Validation mape: %.2f, r2: %.2f'%(epoch+1, mape, r2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HwbAOCSZPM1"
      },
      "source": [
        "n_epochs = 10000 # the prior papar's n_epochs=25\n",
        "train(model, train_loader, val_loader, n_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU1JuOCGcIvN"
      },
      "source": [
        "### 6. **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjQhOCG2Zk1L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
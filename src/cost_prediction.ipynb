{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of cost_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lrqr7SvhlMMw",
        "Hmy3BPKB0Nss",
        "_WjYLaS8YYqW",
        "vJsYbtTZ8QAt",
        "Pz72uokf-LVQ",
        "5utr43ZHPLTd",
        "vv9M-byiPORg",
        "HU1JuOCGcIvN"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbalasub1/CS598IQVIAClaims/blob/main/src/cost_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jej5JXayNs"
      },
      "source": [
        "# **CS598 Deep Learning for Healthcare**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7Q5_3OctOD"
      },
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV62d0RgcLoQ"
      },
      "source": [
        "### 1.1 Change the google colab settings\n",
        "We can use a GPU on the google colab by setting below.  \n",
        "**Edit -> Notebook setting -> Hardware accelerator -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEKo8TAeb9TB"
      },
      "source": [
        "### 1.2 Check if the GPU is available in the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uhApS8Qa2yl",
        "outputId": "1ac81f07-0439-412e-e58d-5f870340f40f"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Device name: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device name: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKcEgpY6ckkX"
      },
      "source": [
        "### 1.3 GPU setting for PyTourch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aCKdPN0bkYo",
        "outputId": "b6bee047-be5b-4fff-b67f-eb5938cf46ad"
      },
      "source": [
        "# The code in this cell is inspired by https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import torch\n",
        "\n",
        "# Tell PyTorch to use the GPU\n",
        "device = torch.device(\"cuda\")\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSDxTfE8dKyo"
      },
      "source": [
        "### 1.4 import necessary packages and setup RNG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN3iA6mgbyNA"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# set seed\n",
        "seed = 100\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QktahRUdTgj"
      },
      "source": [
        "## **2. Dataset loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X87pA2We0fb"
      },
      "source": [
        "### 2.1 Load the IQVIA data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqDmVIIhBA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ac1800-9608-40b0-c2a5-124a41440c64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjt8F4t4kjS5"
      },
      "source": [
        "[Note]\n",
        "Need to upload the iqvia data to your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjGFsD2dPjH"
      },
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/iqvia_data/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqP7CZjfFko"
      },
      "source": [
        "import pandas as pd\n",
        "ENROLL_FILE = DATA_DIR + 'enroll_synth.dat'\n",
        "CLAIMS_2019 = DATA_DIR + 'claims_2019.dat'\n",
        "CLAIMS_2018 = DATA_DIR + 'claims_2018.dat'\n",
        "CLAIMS_2017 = DATA_DIR + 'claims_2017.dat'\n",
        "CLAIMS_2016 = DATA_DIR + 'claims_2016.dat'\n",
        "CLAIMS_2015 = DATA_DIR + 'claims_2015.dat'\n",
        "\n",
        "df_enroll = pd.read_csv(ENROLL_FILE, sep='|', low_memory=False)\n",
        "\n",
        "df_claims2019 = pd.read_csv(CLAIMS_2019, sep='|', low_memory=False)\n",
        "df_claims2018 = pd.read_csv(CLAIMS_2018, sep='|', low_memory=False)\n",
        "df_claims2017 = pd.read_csv(CLAIMS_2017, sep='|', low_memory=False)\n",
        "df_claims2016 = pd.read_csv(CLAIMS_2016, sep='|', low_memory=False)\n",
        "df_claims2015 = pd.read_csv(CLAIMS_2015, sep='|', low_memory=False)\n",
        "\n",
        "## Add year and create a single dataset for claims\n",
        "df_claims2015[\"year\"] = 2015\n",
        "df_claims2016[\"year\"] = 2016\n",
        "df_claims2017[\"year\"] = 2017\n",
        "df_claims2018[\"year\"] = 2018\n",
        "df_claims2019[\"year\"] = 2019\n",
        "\n",
        "list_of_claims = [df_claims2015, df_claims2016, df_claims2017, df_claims2018, df_claims2019]\n",
        "df_claims = pd.concat(list_of_claims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlzdYrkHncNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5ab77819-a8ce-4d7a-e5f2-83d05056593c"
      },
      "source": [
        "# enroll data\n",
        "print(\"Shape of Claims{}\".format(df_enroll.shape))\n",
        "df_enroll.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Claims(30000, 17)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estring</th>\n",
              "      <th>clm_frst</th>\n",
              "      <th>clm_last</th>\n",
              "      <th>nbr_clm_lines</th>\n",
              "      <th>enr_frst</th>\n",
              "      <th>enr_last</th>\n",
              "      <th>mon_totl</th>\n",
              "      <th>mxce_fst</th>\n",
              "      <th>mxce_lst</th>\n",
              "      <th>der_sex</th>\n",
              "      <th>der_yob</th>\n",
              "      <th>pat_id</th>\n",
              "      <th>pat_region</th>\n",
              "      <th>pat_state</th>\n",
              "      <th>grp_indv_cd</th>\n",
              "      <th>mh_cd</th>\n",
              "      <th>enr_rel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8225</th>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>2006-01-10</td>\n",
              "      <td>2017-12-27</td>\n",
              "      <td>417</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>147</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>F</td>\n",
              "      <td>1943.0</td>\n",
              "      <td>f433AAAAAAAGVWTS</td>\n",
              "      <td>E</td>\n",
              "      <td>NY</td>\n",
              "      <td>U</td>\n",
              "      <td>Y</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10794</th>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>2006-06-07</td>\n",
              "      <td>2018-03-05</td>\n",
              "      <td>140</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>147</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>M</td>\n",
              "      <td>1964.0</td>\n",
              "      <td>f433AAAAAAAHRYHG</td>\n",
              "      <td>E</td>\n",
              "      <td>NY</td>\n",
              "      <td>U</td>\n",
              "      <td>Y</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9163</th>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>2007-07-23</td>\n",
              "      <td>2017-03-14</td>\n",
              "      <td>345</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2017-03-31</td>\n",
              "      <td>135</td>\n",
              "      <td>2006-01-01</td>\n",
              "      <td>2017-03-31</td>\n",
              "      <td>F</td>\n",
              "      <td>1960.0</td>\n",
              "      <td>k306AAAAAABKDKXE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>U</td>\n",
              "      <td>Y</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26591</th>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>2016-03-11</td>\n",
              "      <td>28</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>12</td>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>M</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>a7e3AAAAAAPPAPTW</td>\n",
              "      <td>W</td>\n",
              "      <td>CA</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6631</th>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>2017-04-28</td>\n",
              "      <td>2017-05-26</td>\n",
              "      <td>22</td>\n",
              "      <td>2016-03-01</td>\n",
              "      <td>2017-05-31</td>\n",
              "      <td>15</td>\n",
              "      <td>2016-03-01</td>\n",
              "      <td>2017-05-31</td>\n",
              "      <td>F</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>pg20AAAAADXVWYQN</td>\n",
              "      <td>MW</td>\n",
              "      <td>IL</td>\n",
              "      <td>U</td>\n",
              "      <td>N</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 estring  ... enr_rel\n",
              "8225   ----------------------------------------------...  ...      21\n",
              "10794  ----------------------------------------------...  ...      21\n",
              "9163   ----------------------------------------------...  ...      21\n",
              "26591  ----------------------------------------------...  ...      21\n",
              "6631   ----------------------------------------------...  ...      21\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgR9yr9anOQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "c7eec967-ca85-4e60-d96a-89b0f8df3989"
      },
      "source": [
        "# claim data\n",
        "print(\"Shape of Claims{}\".format(df_claims.shape))\n",
        "df_claims.sample(n=5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Claims(2438054, 65)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pat_id</th>\n",
              "      <th>claimno</th>\n",
              "      <th>linenum</th>\n",
              "      <th>rectype</th>\n",
              "      <th>tos_flag</th>\n",
              "      <th>pos</th>\n",
              "      <th>conf_num</th>\n",
              "      <th>patstat</th>\n",
              "      <th>billtype</th>\n",
              "      <th>ndc</th>\n",
              "      <th>daw</th>\n",
              "      <th>formulary</th>\n",
              "      <th>dayssup</th>\n",
              "      <th>quan</th>\n",
              "      <th>proc_cde</th>\n",
              "      <th>cpt_mod</th>\n",
              "      <th>rev_code</th>\n",
              "      <th>srv_unit</th>\n",
              "      <th>from_dt</th>\n",
              "      <th>to_dt</th>\n",
              "      <th>diagprc_ind</th>\n",
              "      <th>diag_admit</th>\n",
              "      <th>diag1</th>\n",
              "      <th>diag2</th>\n",
              "      <th>diag3</th>\n",
              "      <th>diag4</th>\n",
              "      <th>diag5</th>\n",
              "      <th>diag6</th>\n",
              "      <th>diag7</th>\n",
              "      <th>diag8</th>\n",
              "      <th>diag9</th>\n",
              "      <th>diag10</th>\n",
              "      <th>diag11</th>\n",
              "      <th>diag12</th>\n",
              "      <th>icdprc1</th>\n",
              "      <th>icdprc2</th>\n",
              "      <th>icdprc3</th>\n",
              "      <th>icdprc4</th>\n",
              "      <th>icdprc5</th>\n",
              "      <th>icdprc6</th>\n",
              "      <th>icdprc7</th>\n",
              "      <th>icdprc8</th>\n",
              "      <th>icdprc9</th>\n",
              "      <th>icdprc10</th>\n",
              "      <th>icdprc11</th>\n",
              "      <th>icdprc12</th>\n",
              "      <th>charge</th>\n",
              "      <th>allowed</th>\n",
              "      <th>paid</th>\n",
              "      <th>deductible</th>\n",
              "      <th>copay</th>\n",
              "      <th>coinsamt</th>\n",
              "      <th>cobamt</th>\n",
              "      <th>dispense_fee</th>\n",
              "      <th>bill_spec</th>\n",
              "      <th>rend_spec</th>\n",
              "      <th>prscbr_spec</th>\n",
              "      <th>att_spec</th>\n",
              "      <th>pcp_spec</th>\n",
              "      <th>ref_spec</th>\n",
              "      <th>ptypeflg</th>\n",
              "      <th>sub_tp_cd</th>\n",
              "      <th>pmt_st_cd</th>\n",
              "      <th>paid_dt</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>436407</th>\n",
              "      <td>pf06AAAAAAANBFBA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>P</td>\n",
              "      <td>0</td>\n",
              "      <td>01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.78181e+08</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-07-17</td>\n",
              "      <td>2015-07-17</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.22</td>\n",
              "      <td>8.22</td>\n",
              "      <td>6.90</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>U</td>\n",
              "      <td>P</td>\n",
              "      <td>2015-07-22</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102630</th>\n",
              "      <td>a7c7AAAAAAMVLNUZ</td>\n",
              "      <td>BM00016399762</td>\n",
              "      <td>1.0</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99283</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2016-10-17</td>\n",
              "      <td>2016-10-17</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M79672</td>\n",
              "      <td>M79675</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>970.00</td>\n",
              "      <td>970.00</td>\n",
              "      <td>44.60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>U</td>\n",
              "      <td>P</td>\n",
              "      <td>2016-11-16</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552992</th>\n",
              "      <td>pg03AAAAAAQCOAHC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-05-16</td>\n",
              "      <td>2015-05-16</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4659</td>\n",
              "      <td>6869</td>\n",
              "      <td>6929</td>\n",
              "      <td>7862</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U</td>\n",
              "      <td>P</td>\n",
              "      <td>2015-06-15</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141699</th>\n",
              "      <td>9004AAAAAAAIWZJJ</td>\n",
              "      <td>1.93473e+11</td>\n",
              "      <td>1.0</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99212</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2019-11-06</td>\n",
              "      <td>2019-11-06</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M47812</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>133.00</td>\n",
              "      <td>112.20</td>\n",
              "      <td>102.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OTHR_SPC</td>\n",
              "      <td>OTHR_SPC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>U</td>\n",
              "      <td>P</td>\n",
              "      <td>2019-12-06</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355175</th>\n",
              "      <td>pg03AAAAAABZBLAE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>450</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>R55</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3027.00</td>\n",
              "      <td>52.91</td>\n",
              "      <td>52.91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>HOSPITAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U</td>\n",
              "      <td>P</td>\n",
              "      <td>2015-12-02</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  pat_id        claimno  linenum  ... pmt_st_cd     paid_dt  year\n",
              "436407  pf06AAAAAAANBFBA            NaN      1.0  ...         P  2015-07-22  2015\n",
              "102630  a7c7AAAAAAMVLNUZ  BM00016399762      1.0  ...         P  2016-11-16  2016\n",
              "552992  pg03AAAAAAQCOAHC            NaN      1.0  ...         P  2015-06-15  2015\n",
              "141699  9004AAAAAAAIWZJJ    1.93473e+11      1.0  ...         P  2019-12-06  2019\n",
              "355175  pg03AAAAAABZBLAE            NaN      4.0  ...         P  2015-12-02  2015\n",
              "\n",
              "[5 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqr7SvhlMMw"
      },
      "source": [
        "### 2.2 Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cCE5CJtfl96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "c333eaba-ad80-4b8e-ea1a-e6e2e18e46cb"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of patients across regions\n",
        "rd = df_enroll[\"pat_region\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients across regions\")\n",
        "\n",
        "# Distribution of patients gender\n",
        "rd = df_enroll[\"der_sex\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\")\n",
        "rd.set_title(\"Distribution of patients' gender\")\n",
        "\n",
        "# Distribution of Age\n",
        "df_enroll[\"age\"] = 2021 - df_enroll[\"der_yob\"]\n",
        "\n",
        "rd = df_enroll[df_enroll[\"der_yob\"] > 1900][\"age\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of patients' age\")\n",
        "\n",
        "# Get the count of claims paid (and denied)\n",
        "df_claims[\"pmt_st_cd\"].value_counts()\n",
        "\n",
        "# number of diagnosis populated in each claim\n",
        "diag_cols = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \"diag12\"]\n",
        "df_claims[\"num_of_diag\"] = df_claims[diag_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_diag\"].mean()\n",
        "\n",
        "# number of icdprc populated in each claim\n",
        "icdprc_cols=[\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\", \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "df_claims[\"num_of_icdprc\"] = df_claims[icdprc_cols].notnull().sum(axis=1)\n",
        "df_claims[\"num_of_icdprc\"].mean()\n",
        "\n",
        "diag = []\n",
        "for colname in diag_cols:\n",
        "    diag.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(diag)))\n",
        "# 22138\n",
        "\n",
        "prc = []\n",
        "for colname in icdprc_cols:\n",
        "    prc.extend(pd.unique(df_claims[colname]))\n",
        "print(len(np.unique(prc)))\n",
        "# 926\n",
        "\n",
        "\n",
        "# number of claims with same day service\n",
        "sum(df_claims[\"from_dt\"] == df_claims[\"to_dt\"])\n",
        "# 2378556 out of 2438054 i.e. 97.5%\n",
        "\n",
        "# Distribution of charges\n",
        "rd = df_claims[\"charge\"].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution charges\")\n",
        "\n",
        "# Log charges makes more sense\n",
        "# filtering out rows where charges are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"charge\"] > 1][\"charge\"]).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Distribution of Log of Charges\")\n",
        "\n",
        "# Distribution of Paid amounts\n",
        "# filtering out rows where paid are less than 1\n",
        "rd = np.log10(df_claims[df_claims[\"paid\"] > 1][\"paid\"]).plot(kind='hist', bins=25)\n",
        "rd.set_title(\"Distribution of Log of Paid\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Checking the unique number of patients in the datasets\n",
        "print(len(pd.unique(df_enroll['pat_id'])))\n",
        "# 30000\n",
        "print(len(pd.unique(df_claims2015['pat_id'])))\n",
        "# 18927\n",
        "print(len(pd.unique(df_claims2016['pat_id'])))\n",
        "#21483\n",
        "print(len(pd.unique(df_claims2017['pat_id'])))\n",
        "#15190\n",
        "print(len(pd.unique(df_claims2018['pat_id'])))\n",
        "#6445\n",
        "print(len(pd.unique(df_claims2019['pat_id'])))\n",
        "#4884"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22138\n",
            "926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "18927\n",
            "21483\n",
            "15190\n",
            "6445\n",
            "4884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNc02_fxghT"
      },
      "source": [
        "## **3. Data embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmy3BPKB0Nss"
      },
      "source": [
        "### 3.1 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C7rmoqSgYo-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "55dcc09d-f864-4c83-b245-f7c9c71c887a"
      },
      "source": [
        "# Wei's code here\n",
        "#assign quarter to each record and convert 'to_dt' to datetime format\n",
        "df_claims[\"quarter\"] = pd.PeriodIndex(pd.to_datetime(df_claims[\"to_dt\"]), freq = 'Q')\n",
        "df_claims['to_dt'] = pd.to_datetime(df_claims['to_dt'])\n",
        "\n",
        "#find the length of claims for each patient\n",
        "df_claims_length = df_claims[['pat_id','to_dt']].groupby('pat_id').agg({'to_dt':['max','min']}).reset_index() \n",
        "df_claims_length.columns = ['pat_id','max','min']\n",
        "df_claims_length['length'] = df_claims_length['max'] - df_claims_length['min']\n",
        "df_claims_length['length'] = df_claims_length['length'].dt.days\n",
        "avg_claim_length = df_claims_length['length'].mean()\n",
        "min_claim_length = df_claims_length['length'].min()\n",
        "max_claim_length = df_claims_length['length'].max()\n",
        "print(min_claim_length,avg_claim_length,max_claim_length)\n",
        "rd = df_claims_length['length'].plot(kind='hist', bins=15)\n",
        "rd.set_title(\"Frequency of Length\")\n",
        "#most patients's length is less than three years, 12 quarters, so chooce an observation window of 12 quarters\n",
        "\n",
        "#calculate index date = last claim - 270 days (180days as prediction window + 90days as last quarter)\n",
        "claim_indx_date = df_claims[['pat_id','to_dt']].groupby('pat_id').agg({'to_dt':['max']}).reset_index()\n",
        "claim_indx_date.columns = ['pat_id','max']\n",
        "claim_indx_date['indx_date'] = claim_indx_date['max'] - pd.to_timedelta(270,unit='d')\n",
        "#filter claims, \n",
        "filterred_claims = pd.merge(df_claims,claim_indx_date,how = 'left',on=['pat_id'])\n",
        "#observation includes 1000 days(three years) before index date\n",
        "#prediction window is 180 days (two quarters), last 90 days(last quarter) is reserved as target\n",
        "filterred_observation = filterred_claims.loc[filterred_claims.to_dt<=filterred_claims.indx_date]\n",
        "filterred_observation = filterred_observation.loc[filterred_observation.to_dt>=filterred_observation.indx_date-pd.to_timedelta(1095,unit='d')]\n",
        "filterred_observation = filterred_observation.loc[filterred_observation.to_dt>=pd.to_datetime('2015-10-1')] #code changed after October 1, 2015\n",
        "filterred_target = filterred_claims.loc[filterred_claims.to_dt>=filterred_claims.indx_date+pd.to_timedelta(180,unit='d')]\n",
        "\n",
        "#find the number of claims after filter\n",
        "pat_claimsnumb = filterred_observation[['pat_id']].groupby('pat_id').agg({'pat_id':['count']}).reset_index()\n",
        "pat_claimsnumb.columns = ['pat_id','count']\n",
        "print(pat_claimsnumb.shape,pat_claimsnumb['count'].max(),pat_claimsnumb['count'].mean(),pat_claimsnumb['count'].min())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 647.9217659590098 1885\n",
            "(18474, 2) 3257 55.39920970011909 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs1T_C4ixruV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a527cb-57ca-44ec-b7b4-6b0d0987ad6f"
      },
      "source": [
        "diag_cols = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \"diag12\"]\n",
        "icdprc_cols=[\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\", \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "\n",
        "#number of unique diag codes and prc codes after filter\n",
        "diag = []\n",
        "for colname in diag_cols:\n",
        "    diag.extend(pd.unique(filterred_observation[colname]))\n",
        "diag_dict = np.unique(diag)\n",
        "print(len(np.unique(diag)))\n",
        "\n",
        "prc = []\n",
        "for colname in icdprc_cols:\n",
        "    prc.extend(pd.unique(filterred_observation[colname]))\n",
        "prc_dict = np.unique(prc)\n",
        "print(len(np.unique(prc)))\n",
        "\n",
        "#number of unique record type, procedure code and revenue code(high-level description of services)\n",
        "filterred_observation[\"rectype\"] = filterred_observation[\"rectype\"].astype('str')\n",
        "filterred_observation[\"proc_cde\"] = filterred_observation[\"proc_cde\"].astype('str')\n",
        "filterred_observation[\"rev_code\"] = filterred_observation[\"rev_code\"].astype('str')\n",
        "print(len(np.unique(filterred_observation[\"rectype\"])))\n",
        "print(len(np.unique(filterred_observation[\"proc_cde\"])))\n",
        "print(len(np.unique(filterred_observation[\"rev_code\"])))\n",
        "rectype_dict = np.unique(filterred_observation[\"rectype\"])\n",
        "proc_cde_dict = np.unique(filterred_observation[\"proc_cde\"])\n",
        "rev_code_dict = np.unique(filterred_observation[\"rev_code\"])\n",
        "\n",
        "#extract interested columns\n",
        "filtered_features = filterred_observation[[\"pat_id\",\"paid\",\"charge\",\"quarter\",\"rectype\",\"proc_cde\",\"rev_code\",\"diag1\", \"diag2\", \"diag3\", \n",
        "                                           \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\",\"diag12\",\"icdprc1\", \n",
        "                                           \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\",\"icdprc10\",\n",
        "                                           \"icdprc11\", \"icdprc12\"]]\n",
        "\n",
        "print(filtered_features.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12952\n",
            "420\n",
            "6\n",
            "6114\n",
            "718\n",
            "(1023445, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzhtkICX13WX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ab99d89-3f0a-4776-cdfd-fd507ea6809f"
      },
      "source": [
        "#investigate unqiue codes after filter\n",
        "proc_cde_valuecount = filtered_features[\"proc_cde\"].value_counts()\n",
        "rev_code_valuecount = filtered_features[\"rev_code\"].value_counts()\n",
        "diag_valuecount = filtered_features[\"diag1\"].value_counts()\n",
        "prc_valuecount = filtered_features[\"icdprc1\"].value_counts()\n",
        "\n",
        "proc_cde_valuecount = proc_cde_valuecount.to_frame()\n",
        "rd = np.log10(proc_cde_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of proc_cde\")\n",
        "#among 6114, less than 500 proc codes appear more than 100 times\n",
        "rev_code_valuecount = rev_code_valuecount.to_frame()\n",
        "rd = np.log10(rev_code_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of revenue code\")\n",
        "#among 718, less than 300 revenue codes appear more than 100 times\n",
        "diag_valuecount = diag_valuecount.to_frame()\n",
        "rd = np.log10(diag_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of diag code\")\n",
        "#among 12952, about 1000 diag codes appear more than 100 times\n",
        "prc_valuecount = prc_valuecount.to_frame()\n",
        "rd = np.log10(prc_valuecount).plot(kind='hist', bins=15)\n",
        "rd.set_title(\"frequency of prc code\")\n",
        "#among 420, less than 50 appear more than 100 times\n",
        "\n",
        "proc_cde_valuecount = filtered_features[\"proc_cde\"].value_counts().reset_index()\n",
        "rev_code_valuecount = filtered_features[\"rev_code\"].value_counts().reset_index()\n",
        "diag_valuecount = filtered_features[\"diag1\"].value_counts().reset_index()\n",
        "prc_valuecount = filtered_features[\"icdprc1\"].value_counts().reset_index()\n",
        "\n",
        "#apply thresholds on codes to extract dictionary for high frequency codes\n",
        "proc_cde_dict = set(proc_cde_valuecount.apply(lambda x: x['index'] if x['proc_cde']>=1000 else 0,axis=1).to_list())\n",
        "rev_code_dict = set(rev_code_valuecount.apply(lambda x: x['index'] if x['rev_code']>=1000 else 0,axis=1).to_list())\n",
        "diag_dict = set(diag_valuecount.apply(lambda x: x['index'] if x['diag1']>=1000 else 0,axis=1).to_list())\n",
        "prc_dict = set(prc_valuecount.apply(lambda x: x['index'] if x['icdprc1']>=100 else 0,axis=1).to_list())\n",
        "proc_cde_dict.remove(0)\n",
        "rev_code_dict.remove(0)\n",
        "diag_dict.remove(0)\n",
        "prc_dict.remove(0)\n",
        "\n",
        "print(len(proc_cde_dict),len(rev_code_dict),len(diag_dict),len(prc_dict))\n",
        "#total code features are reduced to less than 300 after filtering\n",
        "\n",
        "#filter out code features that not appears in the dictionary\n",
        "def combine_columns_into_list(x,cols,dict_set):\n",
        "    out = list()\n",
        "    for col in cols:\n",
        "        if x[col] in dict_set:\n",
        "            out.append(x[col])\n",
        "    return out\n",
        "\n",
        "filtered_features['diags'] = filtered_features.apply(lambda x: combine_columns_into_list(x,[\"diag1\",\"diag2\",\"diag3\",\"diag4\",\"diag5\",\"diag6\",\"diag7\",\"diag8\",\"diag9\",\"diag10\"],diag_dict),axis=1)\n",
        "filtered_features['rev_code'] = filtered_features.apply(lambda x: x['rev_code'] if x['rev_code'] in rev_code_dict else 0,axis=1)\n",
        "filtered_features['icdprc'] = filtered_features.apply(lambda x: x['icdprc1'] if x['icdprc1'] in prc_dict else 0,axis=1)\n",
        "filtered_features['proc_cde'] = filtered_features.apply(lambda x: x['proc_cde'] if x['proc_cde'] in proc_cde_dict else 0,axis=1)\n",
        "\n",
        "#drop original code columns\n",
        "dropped_features = [\"diag1\", \"diag2\", \"diag3\", \"diag4\", \"diag5\", \"diag6\", \"diag7\", \"diag8\", \"diag9\", \"diag10\", \"diag11\", \n",
        "                \"diag12\",\"icdprc1\", \"icdprc2\", \"icdprc3\", \"icdprc4\", \"icdprc5\", \"icdprc6\", \"icdprc7\", \"icdprc8\", \"icdprc9\",\n",
        "                \"icdprc10\", \"icdprc11\", \"icdprc12\"]\n",
        "filtered_features = filtered_features.drop(dropped_features,axis=1)\n",
        "\n",
        "print(filtered_features.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115 41 105 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1023445, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMY9zCQ_h_xR"
      },
      "source": [
        "filtered_features = filtered_features.drop(['charge'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URl6Z-bJ20d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e8dfc8-85cb-4d22-9e04-973cb2bb2804"
      },
      "source": [
        "#build multi-hot for rectype, proc_cde, rev_code, recvtype and icdprc\n",
        "filtered_features = pd.get_dummies(filtered_features,columns = ['rectype','proc_cde','rev_code','icdprc'])\n",
        "print(filtered_features.shape)\n",
        "filtered_dict_features = filtered_features.copy(deep=True)\n",
        "\n",
        "#build multi-hot for diags list feature\n",
        "v = filtered_dict_features.diags.values\n",
        "l = [len(x) for x in v.tolist()]\n",
        "f, u = pd.factorize(np.concatenate(v))\n",
        "n, m = len(v), u.size\n",
        "i = np.arange(n).repeat(l)\n",
        "\n",
        "dummies = pd.DataFrame(\n",
        "    np.bincount(i * m + f, minlength=n * m).reshape(n, m),\n",
        "    filtered_dict_features.index, u\n",
        ")\n",
        "\n",
        "filtered_dict_features = filtered_dict_features.drop('diags', 1).join(dummies)\n",
        "\n",
        "#filtered_dict_features.to_pickle('./filterd_feature_dict_281.pkl')\n",
        "filtered_dict_features = filtered_dict_features.groupby(['pat_id','quarter']).sum().reset_index()\n",
        "#sort the pat_id and quarter in ascending order\n",
        "filtered_dict_features = filtered_dict_features.sort_values(by=['pat_id','quarter'],ascending=False)\n",
        "\n",
        "print(filtered_dict_features.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1023445, 176)\n",
            "(72814, 280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT9XeY10ieFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e82920-804e-4db9-c7f0-95fa12ba36e5"
      },
      "source": [
        "#filtered_dict_features = filtered_dict_features.drop(['charge'],axis = 1)\n",
        "print(filtered_dict_features.shape)\n",
        "filtered_dict_features.to_csv('./feature_280.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(72814, 280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMMmce4jiXJq"
      },
      "source": [
        "#build pat_id list to extract data from filtered_dict_features\n",
        "pat_ids_dict = set(np.unique(filtered_dict_features['pat_id']))\n",
        "pat_ids_dict = sorted(list(pat_ids_dict),reverse=True)\n",
        "\n",
        "\n",
        "#construct data for model training\n",
        "x = np.zeros((18474,278*12))\n",
        "\n",
        "\n",
        "## construct x matrix\n",
        "num_pat = 0\n",
        "for pat in pat_ids_dict:\n",
        "    pat_val = filtered_dict_features.loc[filtered_dict_features['pat_id']==pat]\n",
        "    pat_val = pat_val.drop(['pat_id','quarter'],axis=1)\n",
        "    raw_values = pat_val.values.tolist()\n",
        "    if len(raw_values)>12:\n",
        "        raw_values = raw_values[:-1] #in case of 13 quarters\n",
        "    values = [item for sublist in raw_values for item in sublist]\n",
        "    x[num_pat,:len(values)] = values\n",
        "    num_pat+=1\n",
        "\n",
        "#np.savetxt(\"x_18474_3348.csv\", x, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vICrkdVa3z5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df47778-26f5-4863-f206-a3e221dd49d8"
      },
      "source": [
        "#build the y vector\n",
        "y_paid = filterred_target[['pat_id','paid']].groupby('pat_id').agg({'paid':['sum']}).reset_index() \n",
        "y_charge = filterred_target[['pat_id','charge']].groupby('pat_id').agg({'charge':['sum']}).reset_index() \n",
        "y_paid.columns = ['pat_id','paid']\n",
        "y_charge.columns = ['pat_id','charge']\n",
        "print(y_paid.shape,y_charge.shape)\n",
        "print(y_paid.isnull().values.any(),y_charge.isnull().values.any())\n",
        "\n",
        "y_charge['miss'] = y_charge.apply(lambda x: 0 if x['pat_id'] in pat_ids_dict else 1,axis=1)\n",
        "y_charge = y_charge.loc[y_charge['miss']!=1]\n",
        "y_charge = y_charge.drop(['miss'],axis=1)\n",
        "y_charge = y_charge.sort_values(by=['pat_id'],ascending = False)\n",
        "\n",
        "y_paid['miss'] = y_paid.apply(lambda x: 0 if x['pat_id'] in pat_ids_dict else 1,axis=1)\n",
        "y_paid = y_paid.loc[y_paid['miss']!=1]\n",
        "y_paid = y_paid.drop(['miss'],axis=1)\n",
        "y_paid = y_paid.sort_values(by=['pat_id'],ascending = False)\n",
        "\n",
        "#patient id is matched row by row between y_paid and x\n",
        "print(y_charge['pat_id'].to_list()==pat_ids_dict)\n",
        "print(y_paid['pat_id'].to_list()==pat_ids_dict)\n",
        "#True True\n",
        "\n",
        "ypaid = np.array(y_paid['paid'].to_list())\n",
        "#np.savetxt(\"ypaid.csv\", ypaid, delimiter=\",\")\n",
        "ycharge = np.array(y_charge['charge'].to_list())\n",
        "#np.savetxt(\"ycharge.csv\", ycharge, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27226, 2) (27226, 2)\n",
            "False False\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wSCphZS5pll"
      },
      "source": [
        "# Load csv data that is already preprocessed by Wei\n",
        "# -> Wei's code should be in this ipynb?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WjYLaS8YYqW"
      },
      "source": [
        "### 3.2 Load prepared data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCk2xBeR61c"
      },
      "source": [
        "#\n",
        "def read_emb_csv(filepath):\n",
        "    #pd_x = pd.read_csv(filepath + 'x_18474_3348.csv')\n",
        "    pd_x = pd.read_csv(filepath + 'x_18474_3336.csv')\n",
        "    pd_y_charge = pd.read_csv(filepath + 'ycharge.csv')\n",
        "    pd_y_paid = pd.read_csv(filepath + 'ypaid.csv')\n",
        "\n",
        "    return pd_x, pd_y_charge, pd_y_paid\n",
        "\n",
        "# load the emb csv\n",
        "EMB_DATA_PATH = '/content/drive/MyDrive/iqvia_data/'\n",
        "pd_x, pd_y_charge, pd_y_paid = read_emb_csv(EMB_DATA_PATH) \n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3cDoh8DhZKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc15b89-c153-46f7-a06c-6e0205992d4c"
      },
      "source": [
        "print(\"pd_x: {}\".format(pd_x.shape))\n",
        "print(\"pd_y_charge: {}\".format(pd_y_charge.shape))\n",
        "print(\"pd_y_paid: {}\".format(pd_y_paid.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pd_x: (18473, 3336)\n",
            "pd_y_charge: (18473, 1)\n",
            "pd_y_paid: (18473, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "xnXvmTXTOTDg",
        "outputId": "a42c3f7d-511f-42b4-f5cb-b5f797458877"
      },
      "source": [
        "pd_y_paid.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5.061299999999999955e+02</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.847300e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.810990e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.460997e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.391000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.042600e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.233300e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.552063e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       5.061299999999999955e+02\n",
              "count              1.847300e+04\n",
              "mean               1.810990e+03\n",
              "std                1.460997e+04\n",
              "min                0.000000e+00\n",
              "25%                9.391000e+01\n",
              "50%                3.042600e+02\n",
              "75%                9.233300e+02\n",
              "max                1.552063e+06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "W68OaeLANnb5",
        "outputId": "90559609-5a6b-4afd-a9d5-c5f48104f9a1"
      },
      "source": [
        "pd_y_paid.hist(bins=100, log=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f5702fc1e90>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5GaZPJkQTYr",
        "outputId": "f6d8c9a5-9b50-46fa-99a7-3ca55dff742a"
      },
      "source": [
        "outlier = np.where(pd_y_paid > 100000)[0]\n",
        "print(outlier)\n",
        "print(outlier.shape)\n",
        "#print(pd_y_paid.loc[np.where(pd_y_paid > 100000)[0]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  194   443  1084  1356  1717  2961  3335  3925  4108  4551  5866  6689\n",
            "  6856  6928  8255 11465 12310 12453 13161 16719]\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ2m5lcRR2gX"
      },
      "source": [
        "# Drop outlier\n",
        "pd_x = pd_x.drop(index=outlier)\n",
        "pd_y_paid = pd_y_paid.drop(index=outlier)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "dz14FTy4aFJu",
        "outputId": "9246c98d-9f34-4392-8cfd-1056989223cf"
      },
      "source": [
        "pd_x.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>3.487200000000000273e+02</th>\n",
              "      <th>0.000000000000000000e+00</th>\n",
              "      <th>0.000000000000000000e+00.1</th>\n",
              "      <th>0.000000000000000000e+00.2</th>\n",
              "      <th>8.000000000000000000e+00</th>\n",
              "      <th>0.000000000000000000e+00.3</th>\n",
              "      <th>1.000000000000000000e+01</th>\n",
              "      <th>0.000000000000000000e+00.4</th>\n",
              "      <th>0.000000000000000000e+00.5</th>\n",
              "      <th>0.000000000000000000e+00.6</th>\n",
              "      <th>0.000000000000000000e+00.7</th>\n",
              "      <th>0.000000000000000000e+00.8</th>\n",
              "      <th>0.000000000000000000e+00.9</th>\n",
              "      <th>0.000000000000000000e+00.10</th>\n",
              "      <th>0.000000000000000000e+00.11</th>\n",
              "      <th>0.000000000000000000e+00.12</th>\n",
              "      <th>0.000000000000000000e+00.13</th>\n",
              "      <th>0.000000000000000000e+00.14</th>\n",
              "      <th>0.000000000000000000e+00.15</th>\n",
              "      <th>0.000000000000000000e+00.16</th>\n",
              "      <th>0.000000000000000000e+00.17</th>\n",
              "      <th>0.000000000000000000e+00.18</th>\n",
              "      <th>0.000000000000000000e+00.19</th>\n",
              "      <th>0.000000000000000000e+00.20</th>\n",
              "      <th>0.000000000000000000e+00.21</th>\n",
              "      <th>0.000000000000000000e+00.22</th>\n",
              "      <th>0.000000000000000000e+00.23</th>\n",
              "      <th>0.000000000000000000e+00.24</th>\n",
              "      <th>0.000000000000000000e+00.25</th>\n",
              "      <th>0.000000000000000000e+00.26</th>\n",
              "      <th>0.000000000000000000e+00.27</th>\n",
              "      <th>0.000000000000000000e+00.28</th>\n",
              "      <th>0.000000000000000000e+00.29</th>\n",
              "      <th>0.000000000000000000e+00.30</th>\n",
              "      <th>0.000000000000000000e+00.31</th>\n",
              "      <th>0.000000000000000000e+00.32</th>\n",
              "      <th>0.000000000000000000e+00.33</th>\n",
              "      <th>0.000000000000000000e+00.34</th>\n",
              "      <th>0.000000000000000000e+00.35</th>\n",
              "      <th>0.000000000000000000e+00.36</th>\n",
              "      <th>...</th>\n",
              "      <th>0.000000000000000000e+00.3245</th>\n",
              "      <th>0.000000000000000000e+00.3246</th>\n",
              "      <th>0.000000000000000000e+00.3247</th>\n",
              "      <th>0.000000000000000000e+00.3248</th>\n",
              "      <th>0.000000000000000000e+00.3249</th>\n",
              "      <th>0.000000000000000000e+00.3250</th>\n",
              "      <th>0.000000000000000000e+00.3251</th>\n",
              "      <th>0.000000000000000000e+00.3252</th>\n",
              "      <th>0.000000000000000000e+00.3253</th>\n",
              "      <th>0.000000000000000000e+00.3254</th>\n",
              "      <th>0.000000000000000000e+00.3255</th>\n",
              "      <th>0.000000000000000000e+00.3256</th>\n",
              "      <th>0.000000000000000000e+00.3257</th>\n",
              "      <th>0.000000000000000000e+00.3258</th>\n",
              "      <th>0.000000000000000000e+00.3259</th>\n",
              "      <th>0.000000000000000000e+00.3260</th>\n",
              "      <th>0.000000000000000000e+00.3261</th>\n",
              "      <th>0.000000000000000000e+00.3262</th>\n",
              "      <th>0.000000000000000000e+00.3263</th>\n",
              "      <th>0.000000000000000000e+00.3264</th>\n",
              "      <th>0.000000000000000000e+00.3265</th>\n",
              "      <th>0.000000000000000000e+00.3266</th>\n",
              "      <th>0.000000000000000000e+00.3267</th>\n",
              "      <th>0.000000000000000000e+00.3268</th>\n",
              "      <th>0.000000000000000000e+00.3269</th>\n",
              "      <th>0.000000000000000000e+00.3270</th>\n",
              "      <th>0.000000000000000000e+00.3271</th>\n",
              "      <th>0.000000000000000000e+00.3272</th>\n",
              "      <th>0.000000000000000000e+00.3273</th>\n",
              "      <th>0.000000000000000000e+00.3274</th>\n",
              "      <th>0.000000000000000000e+00.3275</th>\n",
              "      <th>0.000000000000000000e+00.3276</th>\n",
              "      <th>0.000000000000000000e+00.3277</th>\n",
              "      <th>0.000000000000000000e+00.3278</th>\n",
              "      <th>0.000000000000000000e+00.3279</th>\n",
              "      <th>0.000000000000000000e+00.3280</th>\n",
              "      <th>0.000000000000000000e+00.3281</th>\n",
              "      <th>0.000000000000000000e+00.3282</th>\n",
              "      <th>0.000000000000000000e+00.3283</th>\n",
              "      <th>0.000000000000000000e+00.3284</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.0</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.000000</td>\n",
              "      <td>18453.0</td>\n",
              "      <td>18453.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>927.337872</td>\n",
              "      <td>4.541104</td>\n",
              "      <td>0.023248</td>\n",
              "      <td>2.297187</td>\n",
              "      <td>2.903593</td>\n",
              "      <td>0.224137</td>\n",
              "      <td>0.162846</td>\n",
              "      <td>2.782800</td>\n",
              "      <td>0.149786</td>\n",
              "      <td>0.011543</td>\n",
              "      <td>0.015119</td>\n",
              "      <td>0.032786</td>\n",
              "      <td>0.015770</td>\n",
              "      <td>0.012031</td>\n",
              "      <td>0.011001</td>\n",
              "      <td>0.016366</td>\n",
              "      <td>0.037772</td>\n",
              "      <td>0.027584</td>\n",
              "      <td>0.098304</td>\n",
              "      <td>0.088603</td>\n",
              "      <td>0.012302</td>\n",
              "      <td>0.043841</td>\n",
              "      <td>0.024657</td>\n",
              "      <td>0.032786</td>\n",
              "      <td>0.024386</td>\n",
              "      <td>0.011272</td>\n",
              "      <td>0.031648</td>\n",
              "      <td>0.007858</td>\n",
              "      <td>0.011922</td>\n",
              "      <td>0.014686</td>\n",
              "      <td>0.010730</td>\n",
              "      <td>0.060370</td>\n",
              "      <td>0.011380</td>\n",
              "      <td>0.013602</td>\n",
              "      <td>0.012031</td>\n",
              "      <td>0.012898</td>\n",
              "      <td>0.023628</td>\n",
              "      <td>0.051428</td>\n",
              "      <td>0.009755</td>\n",
              "      <td>0.015661</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005257</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.006503</td>\n",
              "      <td>0.003089</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.004064</td>\n",
              "      <td>0.001843</td>\n",
              "      <td>0.002059</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.005907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.002059</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.005311</td>\n",
              "      <td>0.004064</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.009809</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.005473</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4189.457729</td>\n",
              "      <td>10.351737</td>\n",
              "      <td>0.243607</td>\n",
              "      <td>4.565504</td>\n",
              "      <td>5.394192</td>\n",
              "      <td>0.925029</td>\n",
              "      <td>1.153127</td>\n",
              "      <td>6.833141</td>\n",
              "      <td>0.497119</td>\n",
              "      <td>0.156606</td>\n",
              "      <td>0.234743</td>\n",
              "      <td>0.299863</td>\n",
              "      <td>0.185715</td>\n",
              "      <td>0.163179</td>\n",
              "      <td>0.126819</td>\n",
              "      <td>0.156349</td>\n",
              "      <td>0.281162</td>\n",
              "      <td>0.172484</td>\n",
              "      <td>0.391742</td>\n",
              "      <td>0.300672</td>\n",
              "      <td>0.142408</td>\n",
              "      <td>0.256451</td>\n",
              "      <td>0.181788</td>\n",
              "      <td>0.225845</td>\n",
              "      <td>0.200399</td>\n",
              "      <td>0.109602</td>\n",
              "      <td>0.189914</td>\n",
              "      <td>0.111612</td>\n",
              "      <td>0.117644</td>\n",
              "      <td>0.130661</td>\n",
              "      <td>0.113541</td>\n",
              "      <td>0.260145</td>\n",
              "      <td>0.120878</td>\n",
              "      <td>0.181759</td>\n",
              "      <td>0.173169</td>\n",
              "      <td>0.118459</td>\n",
              "      <td>0.163240</td>\n",
              "      <td>0.239476</td>\n",
              "      <td>0.146541</td>\n",
              "      <td>0.144723</td>\n",
              "      <td>...</td>\n",
              "      <td>0.321942</td>\n",
              "      <td>0.090145</td>\n",
              "      <td>0.059804</td>\n",
              "      <td>0.101182</td>\n",
              "      <td>0.145720</td>\n",
              "      <td>0.203337</td>\n",
              "      <td>0.225437</td>\n",
              "      <td>0.034525</td>\n",
              "      <td>0.253869</td>\n",
              "      <td>0.313095</td>\n",
              "      <td>0.101187</td>\n",
              "      <td>0.425291</td>\n",
              "      <td>0.180429</td>\n",
              "      <td>0.124919</td>\n",
              "      <td>0.132495</td>\n",
              "      <td>0.119819</td>\n",
              "      <td>0.094260</td>\n",
              "      <td>0.201118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.094556</td>\n",
              "      <td>0.007362</td>\n",
              "      <td>0.280882</td>\n",
              "      <td>0.052568</td>\n",
              "      <td>0.189972</td>\n",
              "      <td>0.062025</td>\n",
              "      <td>0.044169</td>\n",
              "      <td>0.540646</td>\n",
              "      <td>0.183112</td>\n",
              "      <td>0.058421</td>\n",
              "      <td>0.130438</td>\n",
              "      <td>0.848266</td>\n",
              "      <td>0.255203</td>\n",
              "      <td>0.014723</td>\n",
              "      <td>0.678893</td>\n",
              "      <td>0.176676</td>\n",
              "      <td>0.093977</td>\n",
              "      <td>0.257653</td>\n",
              "      <td>0.069047</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.620000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>43.930000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>173.430000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>509.780000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>215211.810000</td>\n",
              "      <td>488.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  3336 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       3.487200000000000273e+02  ...  0.000000000000000000e+00.3284\n",
              "count              18453.000000  ...                        18453.0\n",
              "mean                 927.337872  ...                            0.0\n",
              "std                 4189.457729  ...                            0.0\n",
              "min                   -4.620000  ...                            0.0\n",
              "25%                   43.930000  ...                            0.0\n",
              "50%                  173.430000  ...                            0.0\n",
              "75%                  509.780000  ...                            0.0\n",
              "max               215211.810000  ...                            0.0\n",
              "\n",
              "[8 rows x 3336 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "KY0J4zJvnRVV",
        "outputId": "45dfb5d1-b1cb-44a6-bbff-611b54b933be"
      },
      "source": [
        "pd_y_paid.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5.061299999999999955e+02</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>18453.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1525.336082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5090.648841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>93.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>303.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>918.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>97888.740000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       5.061299999999999955e+02\n",
              "count              18453.000000\n",
              "mean                1525.336082\n",
              "std                 5090.648841\n",
              "min                    0.000000\n",
              "25%                   93.660000\n",
              "50%                  303.660000\n",
              "75%                  918.120000\n",
              "max                97888.740000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "9RA08qE4bRZZ",
        "outputId": "5d820e42-9d4d-4816-c296-a347555ecd55"
      },
      "source": [
        "pd_y_paid.hist(bins=100, log=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f5702f9f0d0>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBRh2V5KiHqh"
      },
      "source": [
        "### X variable's normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLGZrWoxnC-"
      },
      "source": [
        "# Input data normalization\n",
        "# https://betashort-lab.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%AD%A3%E8%A6%8F%E5%8C%96%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86/#toc4\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "if 1:\n",
        "    scaler_mm = MinMaxScaler()\n",
        "    scaler_mm.fit(pd_x)\n",
        "    pd_x = pd.DataFrame(scaler_mm.transform(pd_x))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOhYhvn1nOf5"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        # convert to torch.tensor\n",
        "        self.x = torch.tensor(x.values.astype(np.float32))\n",
        "        self.y = torch.tensor(y.values.astype(np.float32))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        #return self.x.loc[index][:], self.y.loc[index]\n",
        "        return self.x[index][:], self.y[index]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28Z3vjUny3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd6a8d6-c37f-4992-8e39-5c94d53ee15a"
      },
      "source": [
        "dataset = CustomDataset(pd_x, pd_y_paid)\n",
        "print(len(dataset))\n",
        "#print(dataset[0][0])\n",
        "#print(dataset[0][0].shape)\n",
        "#print(dataset[0][1].shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apglZFjRkFfH"
      },
      "source": [
        "### 3.2 Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ewGGH8mH7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50cb736-5633-48f7-f192-f8fe862a0b54"
      },
      "source": [
        "# https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_len = int(len(dataset)*0.7)\n",
        "val_len = int(len(dataset)*0.2)\n",
        "lengths = [train_len, val_len, len(dataset) - train_len - val_len]\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"train data length: {}\".format(len(train_dataset)))\n",
        "print(\"val data length: {}\".format(len(val_dataset)))\n",
        "print(\"test data length: {}\".format(len(test_dataset)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data length: 12917\n",
            "val data length: 3690\n",
            "test data length: 1846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAf9Ps830gPc"
      },
      "source": [
        "### 3.3 Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMMgsNo6SNb"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik5rGVmiOSIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897a5ef7-ce1f-44c4-f3a2-f9064e3af1a5"
      },
      "source": [
        "# Test\n",
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "x, y = next(train_iter)\n",
        "\n",
        "print('Shape of a batch x:', x.shape)\n",
        "print('Shape of a batch y:', y.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of a batch x: torch.Size([32, 3336])\n",
            "Shape of a batch y: torch.Size([32, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gOXYHqyqPI"
      },
      "source": [
        "## **4. Model building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnqKGxz_9ku-"
      },
      "source": [
        "### **4.1 Random Forest and Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yocN3DEg93O7"
      },
      "source": [
        "## Random forest - train on \"train_dataset\", and test on \"test_dataset\"\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import numpy\n",
        "train_x = train_dataset.dataset.x\n",
        "train_y = train_dataset.dataset.y.ravel().cpu().detach().numpy()\n",
        "test_x  = test_dataset.dataset.x\n",
        "test_y  = test_dataset.dataset.y.ravel().cpu().detach().numpy()\n",
        "# filter out rows with Y>100k\n",
        "tf = train_y <= 100e3\n",
        "train_x = train_x[tf, :]\n",
        "train_y = train_y[tf]\n",
        "tf = test_y <= 100e3\n",
        "test_x = test_x[tf, :]\n",
        "test_y = test_y[tf]\n",
        "\n",
        "# fit random forest\n",
        "rf = RandomForestRegressor(max_depth=20, random_state=0)\n",
        "rf.fit(train_x, train_y)\n",
        "f = rf.predict(test_x)\n",
        "abserr = numpy.abs(f - test_y)\n",
        "rf_mae = numpy.mean(abserr)\n",
        "rf_medae = numpy.median(abserr)\n",
        "print(f\"Mean abs err (Random Forest): {rf_mae}\")\n",
        "print(f\"Median abs err (Random Forest): {rf_medae}\")\n",
        "\n",
        "## Gradient Boost\n",
        "gb = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, subsample=0.5)\n",
        "gb.fit(train_x, train_y)\n",
        "f = gb.predict(test_x)\n",
        "abserr = numpy.abs(f - test_y)\n",
        "gb_mae = numpy.mean(abserr)\n",
        "gb_medae = numpy.median(abserr)\n",
        "print(f\"Mean abs err (Gradient Boost): {gb_mae}\")\n",
        "print(f\"Median abs err (Gradient Boost): {gb_medae}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99jHro04qCk0"
      },
      "source": [
        "train_x = train_dataset.dataset.x\n",
        "train_y = train_dataset.dataset.y.ravel().cpu().detach().numpy()\n",
        "test_x  = test_dataset.dataset.x\n",
        "test_y  = test_dataset.dataset.y.ravel().cpu().detach().numpy()\n",
        "# filter out rows with Y>100k\n",
        "tf = train_y <= 100e3\n",
        "train_x = train_x[tf, :]\n",
        "train_y = train_y[tf]\n",
        "tf = test_y <= 100e3\n",
        "test_x = test_x[tf, :]\n",
        "test_y = test_y[tf]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2zQ5qeuqCXD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJsYbtTZ8QAt"
      },
      "source": [
        "### **4.2 Base model(Drewe-Bosss paper)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPcItHq0ys_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b377cf72-6664-440b-88d4-00da46244a52"
      },
      "source": [
        "# The purpose of this model is to reproduce the prior model proposed by Drewe-Boss et al.  Deep learning for prediction of population health costs\n",
        "\n",
        "# This code is inspired by \n",
        "#  (1) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "#  (2) https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/VNfPA/homework-4-mina/lab\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Basemodel(nn.Module):\n",
        "    def __init__(self, input_size=3336, output_size=1):\n",
        "        super(Basemodel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = 50\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(in_features=self.hidden_size+self.input_size, out_features=self.output_size)\n",
        "        self.do = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x = self.do(F.relu(self.fc1(x)))\n",
        "        x = self.do(F.relu(self.fc2(x)))\n",
        "        x = self.do(F.relu(self.fc3(x)))\n",
        "        #print(x.shape)\n",
        "        #print(x0.shape)\n",
        "        x = torch.cat((x, x0), dim=1)\n",
        "        #print(x.shape)\n",
        "        if 1:\n",
        "            x = self.do(F.relu(self.fc4(x)))\n",
        "        else:\n",
        "            #x = self.fc4(x)\n",
        "            x = F.relu(self.fc4(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Basemodel() # Need to specify the input/output size(we have not decided input and output data)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Basemodel(\n",
            "  (fc1): Linear(in_features=3336, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (fc4): Linear(in_features=3386, out_features=1, bias=True)\n",
            "  (do): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz72uokf-LVQ"
      },
      "source": [
        "### **4.3 Advanced model(Base model + alpha)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr8bwEYL_E15"
      },
      "source": [
        "# Idea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upr9qp1AOpTV"
      },
      "source": [
        "## 5. **Model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvR5NLUEO0KQ"
      },
      "source": [
        "### 5.1 Loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N0LMlIOyB4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "8fcaeefe-101b-48f3-d2c0-0833abcec7bd"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Learning rate is not written in the prior papar. => Experimental value\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aba694d62076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Learning rate is not written in the prior papar. => Experimental value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5utr43ZHPLTd"
      },
      "source": [
        "### 5.2 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZLUpgwPRH1"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/UEdCb/homework-2-neural-networks/lab\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "\n",
        "from sklearn.metrics import *\n",
        "\n",
        "def regression_metrics(Y_pred, Y_True):\n",
        "    # Evaluation of methods: \n",
        "    # 1. Pearson's correlation (r), \n",
        "    # 2. Spearman's correlation (\u001a),\n",
        "    # 3. Mean absolute prediction error (MAPE),\n",
        "    # 4. R squared (r2),\n",
        "    # 5. Cumming's Prediction Measure (CPM)\n",
        "    mae, r2 = mean_absolute_error(Y_True, Y_pred), \\\n",
        "                r2_score(Y_True, Y_pred)\n",
        "\n",
        "    return mae, r2\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    all_y_true = torch.LongTensor()\n",
        "    all_y_pred = torch.LongTensor()\n",
        "\n",
        "    val_loss = 0\n",
        "    for x, y in val_loader:\n",
        "        y_pred = model(x)\n",
        "\n",
        "        # convert shape from [batch size, 1] to [batch size]\n",
        "        y_pred = y_pred.view(y_pred.shape[0])\n",
        "        y = y.view(y.shape[0])\n",
        "        \n",
        "        all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
        "        all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        val_loss += loss.item()\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    mae, r2 = regression_metrics(all_y_pred, all_y_true)\n",
        "    #print(f\"mape: {mape:.3f}, r2: {r2:.3f}\")\n",
        "    return val_loss, mae, r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ed2StcLIMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ed84a3-a3ec-4e51-f307-aa82d70b5323"
      },
      "source": [
        "# test without training\n",
        "evaluate(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28378967.019569926, 1535.1652086397771, -0.09050749642407663)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9M-byiPORg"
      },
      "source": [
        "### 5.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4S1IXYPQV3"
      },
      "source": [
        "# https://www.coursera.org/learn/cs598-deep-learning-for-healthcare/programming/eJFr3/homework-3-seq2seq/lab\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss= 0\n",
        "        all_y_true = torch.LongTensor()\n",
        "        all_y_pred = torch.LongTensor()\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "\n",
        "            #print(y_pred.shape)\n",
        "\n",
        "            # convert shape from [batch size, 1] to [batch size]\n",
        "            y_pred = y_pred.view(y_pred.shape[0])\n",
        "            y_pred[y_pred < 0] = 0\n",
        "\n",
        "            y = y.view(y.shape[0])\n",
        "            #print(y_pred.shape)\n",
        "            #print(y.shape)\n",
        "            all_y_true = torch.cat((all_y_true, y.to('cpu').long()), dim=0)\n",
        "            all_y_pred = torch.cat((all_y_pred, y_pred.to('cpu').long()), dim=0)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        #train_MAE = mean_absolute_error(all_y_true, all_y_pred)\n",
        "        _, train_MAE, _ = evaluate(model, train_loader)\n",
        "        val_loss, val_MAE, r2 = evaluate(model, val_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f} \\t Training MAE: {:.6f} \\t Validation Loss: {:.6f} \\t Validation MAE: {:.6f}'.format(epoch+1, train_loss, train_MAE, val_loss, val_MAE))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HwbAOCSZPM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e027c49-ccd3-47ba-c3ab-31f3049c7c53"
      },
      "source": [
        "n_epochs = 300 # the prior papar's n_epochs=25\n",
        "train(model, train_loader, val_loader, n_epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Training Loss: 25875080.929842 \t Training MAE: 1488.850043 \t Validation Loss: 23569218.645474 \t Validation MAE: 1488.586721\n",
            "Epoch: 2 \t Training Loss: 23049188.508973 \t Training MAE: 1599.951614 \t Validation Loss: 22791417.934806 \t Validation MAE: 1714.511111\n",
            "Epoch: 3 \t Training Loss: 20180236.290532 \t Training MAE: 1690.420454 \t Validation Loss: 24160910.111530 \t Validation MAE: 1999.241192\n",
            "Epoch: 4 \t Training Loss: 16956810.251856 \t Training MAE: 1280.517690 \t Validation Loss: 24402985.523168 \t Validation MAE: 1660.413821\n",
            "Epoch: 5 \t Training Loss: 14807663.429378 \t Training MAE: 1266.240768 \t Validation Loss: 26714268.419720 \t Validation MAE: 1810.414092\n",
            "Epoch: 6 \t Training Loss: 12911645.200418 \t Training MAE: 1262.621584 \t Validation Loss: 26185398.591595 \t Validation MAE: 1810.282385\n",
            "Epoch: 7 \t Training Loss: 11629840.440981 \t Training MAE: 1146.756213 \t Validation Loss: 33258137.062500 \t Validation MAE: 1893.515718\n",
            "Epoch: 8 \t Training Loss: 10975123.249923 \t Training MAE: 1114.340869 \t Validation Loss: 32123211.602371 \t Validation MAE: 1890.638753\n",
            "Epoch: 9 \t Training Loss: 10094115.274134 \t Training MAE: 1049.211969 \t Validation Loss: 36099502.048491 \t Validation MAE: 1927.903523\n",
            "Epoch: 10 \t Training Loss: 9297220.100170 \t Training MAE: 962.391345 \t Validation Loss: 34709543.734375 \t Validation MAE: 1759.554743\n",
            "Epoch: 11 \t Training Loss: 8740508.669477 \t Training MAE: 964.268251 \t Validation Loss: 37848352.811422 \t Validation MAE: 1877.528455\n",
            "Epoch: 12 \t Training Loss: 8013859.995514 \t Training MAE: 950.332508 \t Validation Loss: 42424221.181034 \t Validation MAE: 1921.432791\n",
            "Epoch: 13 \t Training Loss: 7695234.616955 \t Training MAE: 953.671441 \t Validation Loss: 41319012.225216 \t Validation MAE: 1938.417344\n",
            "Epoch: 14 \t Training Loss: 7523212.906095 \t Training MAE: 970.342185 \t Validation Loss: 42343153.509698 \t Validation MAE: 2002.625745\n",
            "Epoch: 15 \t Training Loss: 6901812.479076 \t Training MAE: 946.974375 \t Validation Loss: 51191289.887931 \t Validation MAE: 2056.823306\n",
            "Epoch: 16 \t Training Loss: 6795321.542582 \t Training MAE: 912.521793 \t Validation Loss: 49477898.425647 \t Validation MAE: 2065.774255\n",
            "Epoch: 17 \t Training Loss: 6215828.793936 \t Training MAE: 834.508013 \t Validation Loss: 47885694.772629 \t Validation MAE: 1941.541192\n",
            "Epoch: 18 \t Training Loss: 5672879.110613 \t Training MAE: 809.345514 \t Validation Loss: 49958027.060884 \t Validation MAE: 1933.707317\n",
            "Epoch: 19 \t Training Loss: 5590293.005608 \t Training MAE: 879.953317 \t Validation Loss: 55920898.450431 \t Validation MAE: 2109.289702\n",
            "Epoch: 20 \t Training Loss: 5545288.384127 \t Training MAE: 855.999923 \t Validation Loss: 60283984.553879 \t Validation MAE: 2130.971545\n",
            "Epoch: 21 \t Training Loss: 5456447.251586 \t Training MAE: 800.614849 \t Validation Loss: 61877107.273707 \t Validation MAE: 2089.201355\n",
            "Epoch: 22 \t Training Loss: 5272674.234916 \t Training MAE: 771.259116 \t Validation Loss: 57384616.299030 \t Validation MAE: 1975.037940\n",
            "Epoch: 23 \t Training Loss: 5138367.276841 \t Training MAE: 776.421305 \t Validation Loss: 65070079.759698 \t Validation MAE: 2145.628184\n",
            "Epoch: 24 \t Training Loss: 4961362.053218 \t Training MAE: 770.277154 \t Validation Loss: 54702978.576509 \t Validation MAE: 1974.204878\n",
            "Epoch: 25 \t Training Loss: 5217737.980507 \t Training MAE: 757.367732 \t Validation Loss: 61329380.686422 \t Validation MAE: 2050.577507\n",
            "Epoch: 26 \t Training Loss: 4844621.815787 \t Training MAE: 738.907022 \t Validation Loss: 62026344.436422 \t Validation MAE: 2090.039295\n",
            "Epoch: 27 \t Training Loss: 4591360.771388 \t Training MAE: 696.520167 \t Validation Loss: 62018239.880388 \t Validation MAE: 2042.426829\n",
            "Epoch: 28 \t Training Loss: 4374459.208075 \t Training MAE: 705.790818 \t Validation Loss: 70102101.997845 \t Validation MAE: 2176.222493\n",
            "Epoch: 29 \t Training Loss: 4446918.853960 \t Training MAE: 714.037547 \t Validation Loss: 67644616.052802 \t Validation MAE: 2154.216531\n",
            "Epoch: 30 \t Training Loss: 4388105.431544 \t Training MAE: 699.457459 \t Validation Loss: 66111597.232759 \t Validation MAE: 2137.861518\n",
            "Epoch: 31 \t Training Loss: 4367313.051767 \t Training MAE: 731.225749 \t Validation Loss: 66881080.786638 \t Validation MAE: 2152.745799\n",
            "Epoch: 32 \t Training Loss: 4226823.652305 \t Training MAE: 741.330959 \t Validation Loss: 75228277.485991 \t Validation MAE: 2257.123306\n",
            "Epoch: 33 \t Training Loss: 4166459.808014 \t Training MAE: 679.742278 \t Validation Loss: 68171870.008621 \t Validation MAE: 2170.128997\n",
            "Epoch: 34 \t Training Loss: 4080649.552290 \t Training MAE: 713.099946 \t Validation Loss: 74907206.285560 \t Validation MAE: 2238.537940\n",
            "Epoch: 35 \t Training Loss: 4146705.235419 \t Training MAE: 684.175428 \t Validation Loss: 75779166.546336 \t Validation MAE: 2224.533604\n",
            "Epoch: 36 \t Training Loss: 4096188.233776 \t Training MAE: 660.974685 \t Validation Loss: 71329377.103448 \t Validation MAE: 2193.181572\n",
            "Epoch: 37 \t Training Loss: 3895121.030612 \t Training MAE: 645.444221 \t Validation Loss: 70517031.468750 \t Validation MAE: 2186.837940\n",
            "Epoch: 38 \t Training Loss: 3826189.669090 \t Training MAE: 649.294341 \t Validation Loss: 77337501.491379 \t Validation MAE: 2203.650949\n",
            "Epoch: 39 \t Training Loss: 3772220.296392 \t Training MAE: 686.387629 \t Validation Loss: 71444233.264009 \t Validation MAE: 2246.354743\n",
            "Epoch: 40 \t Training Loss: 3867194.160234 \t Training MAE: 639.124023 \t Validation Loss: 72098899.071121 \t Validation MAE: 2155.789973\n",
            "Epoch: 41 \t Training Loss: 3760662.572246 \t Training MAE: 649.805218 \t Validation Loss: 76665321.205819 \t Validation MAE: 2200.122493\n",
            "Epoch: 42 \t Training Loss: 3772940.102684 \t Training MAE: 624.831230 \t Validation Loss: 72791065.696121 \t Validation MAE: 2198.347696\n",
            "Epoch: 43 \t Training Loss: 3686482.535079 \t Training MAE: 615.011380 \t Validation Loss: 73907383.707974 \t Validation MAE: 2194.255556\n",
            "Epoch: 44 \t Training Loss: 3555432.184522 \t Training MAE: 614.410002 \t Validation Loss: 72489974.864224 \t Validation MAE: 2174.921138\n",
            "Epoch: 45 \t Training Loss: 3512649.206451 \t Training MAE: 589.343191 \t Validation Loss: 75457449.938578 \t Validation MAE: 2203.387263\n",
            "Epoch: 46 \t Training Loss: 3498059.356513 \t Training MAE: 590.589611 \t Validation Loss: 70025450.505388 \t Validation MAE: 2164.840379\n",
            "Epoch: 47 \t Training Loss: 3487481.585280 \t Training MAE: 618.970891 \t Validation Loss: 70872539.971983 \t Validation MAE: 2165.507046\n",
            "Epoch: 48 \t Training Loss: 3458243.840733 \t Training MAE: 722.234575 \t Validation Loss: 86880369.394397 \t Validation MAE: 2425.720867\n",
            "Epoch: 49 \t Training Loss: 3458411.341120 \t Training MAE: 602.875823 \t Validation Loss: 77962385.661638 \t Validation MAE: 2198.378862\n",
            "Epoch: 50 \t Training Loss: 3455943.439550 \t Training MAE: 618.661067 \t Validation Loss: 75868674.909483 \t Validation MAE: 2301.034688\n",
            "Epoch: 51 \t Training Loss: 3328760.343112 \t Training MAE: 653.362700 \t Validation Loss: 74468284.743534 \t Validation MAE: 2331.014905\n",
            "Epoch: 52 \t Training Loss: 3248054.534054 \t Training MAE: 575.428892 \t Validation Loss: 72137702.096983 \t Validation MAE: 2211.604607\n",
            "Epoch: 53 \t Training Loss: 3343391.217493 \t Training MAE: 571.480994 \t Validation Loss: 66398082.987069 \t Validation MAE: 2136.289702\n",
            "Epoch: 54 \t Training Loss: 3250472.597946 \t Training MAE: 574.947588 \t Validation Loss: 70651481.428879 \t Validation MAE: 2226.796206\n",
            "Epoch: 55 \t Training Loss: 3171914.891418 \t Training MAE: 657.942247 \t Validation Loss: 73379044.463362 \t Validation MAE: 2353.619783\n",
            "Epoch: 56 \t Training Loss: 3175218.342493 \t Training MAE: 580.383526 \t Validation Loss: 73828137.760776 \t Validation MAE: 2211.821951\n",
            "Epoch: 57 \t Training Loss: 3290407.478342 \t Training MAE: 538.894790 \t Validation Loss: 72343322.393319 \t Validation MAE: 2229.583198\n",
            "Epoch: 58 \t Training Loss: 3126754.299466 \t Training MAE: 606.001316 \t Validation Loss: 72212044.398707 \t Validation MAE: 2325.728455\n",
            "Epoch: 59 \t Training Loss: 3047122.469543 \t Training MAE: 586.694666 \t Validation Loss: 79976386.994612 \t Validation MAE: 2369.764228\n",
            "Epoch: 60 \t Training Loss: 3088077.466623 \t Training MAE: 550.955330 \t Validation Loss: 72120098.084052 \t Validation MAE: 2240.140650\n",
            "Epoch: 61 \t Training Loss: 3148945.887918 \t Training MAE: 571.486568 \t Validation Loss: 65155541.284483 \t Validation MAE: 2217.350949\n",
            "Epoch: 62 \t Training Loss: 3052481.697759 \t Training MAE: 558.045057 \t Validation Loss: 76597050.161638 \t Validation MAE: 2284.808943\n",
            "Epoch: 63 \t Training Loss: 2992989.813796 \t Training MAE: 532.061779 \t Validation Loss: 68626819.913793 \t Validation MAE: 2239.400813\n",
            "Epoch: 64 \t Training Loss: 2884086.590056 \t Training MAE: 578.269722 \t Validation Loss: 76836836.420259 \t Validation MAE: 2380.542547\n",
            "Epoch: 65 \t Training Loss: 2998163.795541 \t Training MAE: 611.462182 \t Validation Loss: 76480412.898707 \t Validation MAE: 2392.987263\n",
            "Epoch: 66 \t Training Loss: 2997732.501044 \t Training MAE: 537.041186 \t Validation Loss: 71564822.719828 \t Validation MAE: 2256.952846\n",
            "Epoch: 67 \t Training Loss: 2884625.965675 \t Training MAE: 522.818147 \t Validation Loss: 67055934.109914 \t Validation MAE: 2219.613821\n",
            "Epoch: 68 \t Training Loss: 2900966.073735 \t Training MAE: 530.404815 \t Validation Loss: 73051600.441810 \t Validation MAE: 2337.208943\n",
            "Epoch: 69 \t Training Loss: 2843576.291112 \t Training MAE: 520.296973 \t Validation Loss: 63950754.129310 \t Validation MAE: 2175.798645\n",
            "Epoch: 70 \t Training Loss: 2801973.471805 \t Training MAE: 559.956104 \t Validation Loss: 75325443.315733 \t Validation MAE: 2351.066667\n",
            "Epoch: 71 \t Training Loss: 2872799.292833 \t Training MAE: 551.393125 \t Validation Loss: 72074061.355603 \t Validation MAE: 2240.860976\n",
            "Epoch: 72 \t Training Loss: 2941763.889890 \t Training MAE: 511.786173 \t Validation Loss: 68605981.120690 \t Validation MAE: 2275.447425\n",
            "Epoch: 73 \t Training Loss: 2808325.486560 \t Training MAE: 486.026709 \t Validation Loss: 70137576.528017 \t Validation MAE: 2226.122493\n",
            "Epoch: 74 \t Training Loss: 2695793.615621 \t Training MAE: 515.186034 \t Validation Loss: 62053068.806034 \t Validation MAE: 2217.111653\n",
            "Epoch: 75 \t Training Loss: 2704698.143468 \t Training MAE: 521.268329 \t Validation Loss: 74413272.118534 \t Validation MAE: 2313.134959\n",
            "Epoch: 76 \t Training Loss: 2721621.071840 \t Training MAE: 517.351939 \t Validation Loss: 70759229.808190 \t Validation MAE: 2280.861518\n",
            "Epoch: 77 \t Training Loss: 2685731.301400 \t Training MAE: 516.379268 \t Validation Loss: 68921009.418103 \t Validation MAE: 2280.357724\n",
            "Epoch: 78 \t Training Loss: 2725442.438022 \t Training MAE: 512.238755 \t Validation Loss: 66918103.849138 \t Validation MAE: 2268.084011\n",
            "Epoch: 79 \t Training Loss: 2681824.191890 \t Training MAE: 477.643493 \t Validation Loss: 65703427.627155 \t Validation MAE: 2220.866667\n",
            "Epoch: 80 \t Training Loss: 2616922.851601 \t Training MAE: 482.780367 \t Validation Loss: 73546952.482759 \t Validation MAE: 2246.031978\n",
            "Epoch: 81 \t Training Loss: 2607688.008905 \t Training MAE: 479.756987 \t Validation Loss: 72734410.704741 \t Validation MAE: 2338.169648\n",
            "Epoch: 82 \t Training Loss: 2590935.966507 \t Training MAE: 479.096385 \t Validation Loss: 64566709.452586 \t Validation MAE: 2203.483740\n",
            "Epoch: 83 \t Training Loss: 2566494.949934 \t Training MAE: 488.144848 \t Validation Loss: 68325618.941810 \t Validation MAE: 2227.838211\n",
            "Epoch: 84 \t Training Loss: 2546852.226040 \t Training MAE: 477.439034 \t Validation Loss: 65184200.790948 \t Validation MAE: 2240.043902\n",
            "Epoch: 85 \t Training Loss: 2527993.935373 \t Training MAE: 488.648370 \t Validation Loss: 70439178.721983 \t Validation MAE: 2292.986992\n",
            "Epoch: 86 \t Training Loss: 2526941.946028 \t Training MAE: 495.459859 \t Validation Loss: 68114054.103448 \t Validation MAE: 2313.918970\n",
            "Epoch: 87 \t Training Loss: 2426472.390374 \t Training MAE: 474.625068 \t Validation Loss: 67110903.379310 \t Validation MAE: 2301.028997\n",
            "Epoch: 88 \t Training Loss: 2420749.702003 \t Training MAE: 475.436711 \t Validation Loss: 71491452.946121 \t Validation MAE: 2337.969106\n",
            "Epoch: 89 \t Training Loss: 2438047.387976 \t Training MAE: 500.442827 \t Validation Loss: 59916972.877155 \t Validation MAE: 2156.924119\n",
            "Epoch: 90 \t Training Loss: 2579467.622563 \t Training MAE: 509.741349 \t Validation Loss: 62215281.183190 \t Validation MAE: 2251.025745\n",
            "Epoch: 91 \t Training Loss: 2508414.160408 \t Training MAE: 477.343346 \t Validation Loss: 64157481.911638 \t Validation MAE: 2283.712195\n",
            "Epoch: 92 \t Training Loss: 2428021.209255 \t Training MAE: 480.526980 \t Validation Loss: 62332856.515086 \t Validation MAE: 2275.543089\n",
            "Epoch: 93 \t Training Loss: 2330284.957882 \t Training MAE: 447.842378 \t Validation Loss: 64985764.122845 \t Validation MAE: 2273.398374\n",
            "Epoch: 94 \t Training Loss: 2297395.804397 \t Training MAE: 515.110629 \t Validation Loss: 70283413.965517 \t Validation MAE: 2377.241463\n",
            "Epoch: 95 \t Training Loss: 2321664.468383 \t Training MAE: 470.113416 \t Validation Loss: 69385960.687500 \t Validation MAE: 2362.442547\n",
            "Epoch: 96 \t Training Loss: 2311301.765480 \t Training MAE: 450.941937 \t Validation Loss: 60952339.594828 \t Validation MAE: 2202.202710\n",
            "Epoch: 97 \t Training Loss: 2313120.456132 \t Training MAE: 477.121623 \t Validation Loss: 64920462.265086 \t Validation MAE: 2245.184011\n",
            "Epoch: 98 \t Training Loss: 2291109.885597 \t Training MAE: 442.361384 \t Validation Loss: 60755027.387931 \t Validation MAE: 2214.491328\n",
            "Epoch: 99 \t Training Loss: 2267263.576868 \t Training MAE: 441.165518 \t Validation Loss: 63867324.584052 \t Validation MAE: 2228.164228\n",
            "Epoch: 100 \t Training Loss: 2220968.745311 \t Training MAE: 450.541999 \t Validation Loss: 70580236.859914 \t Validation MAE: 2351.033062\n",
            "Epoch: 101 \t Training Loss: 2260352.632900 \t Training MAE: 616.604862 \t Validation Loss: 72248481.665948 \t Validation MAE: 2556.597019\n",
            "Epoch: 102 \t Training Loss: 2389856.045492 \t Training MAE: 472.879384 \t Validation Loss: 68198642.685345 \t Validation MAE: 2370.717886\n",
            "Epoch: 103 \t Training Loss: 2229935.236899 \t Training MAE: 446.967794 \t Validation Loss: 63275140.116379 \t Validation MAE: 2226.666667\n",
            "Epoch: 104 \t Training Loss: 2156439.555364 \t Training MAE: 528.699853 \t Validation Loss: 67453469.622845 \t Validation MAE: 2485.233604\n",
            "Epoch: 105 \t Training Loss: 2117657.110342 \t Training MAE: 446.902144 \t Validation Loss: 67362779.284483 \t Validation MAE: 2343.012195\n",
            "Epoch: 106 \t Training Loss: 2082871.583250 \t Training MAE: 438.646667 \t Validation Loss: 62671688.426724 \t Validation MAE: 2316.559350\n",
            "Epoch: 107 \t Training Loss: 2154280.826675 \t Training MAE: 463.198653 \t Validation Loss: 67271359.476293 \t Validation MAE: 2306.741463\n",
            "Epoch: 108 \t Training Loss: 2280571.626644 \t Training MAE: 461.139893 \t Validation Loss: 53824251.218750 \t Validation MAE: 2187.726829\n",
            "Epoch: 109 \t Training Loss: 2269377.570467 \t Training MAE: 448.477123 \t Validation Loss: 65624632.870690 \t Validation MAE: 2310.478862\n",
            "Epoch: 110 \t Training Loss: 2063412.529780 \t Training MAE: 420.451343 \t Validation Loss: 66228373.448276 \t Validation MAE: 2320.891057\n",
            "Epoch: 111 \t Training Loss: 2015276.974493 \t Training MAE: 414.106217 \t Validation Loss: 64704567.943966 \t Validation MAE: 2290.169648\n",
            "Epoch: 112 \t Training Loss: 2051853.238030 \t Training MAE: 433.001935 \t Validation Loss: 59608951.306034 \t Validation MAE: 2277.720054\n",
            "Epoch: 113 \t Training Loss: 2098447.944491 \t Training MAE: 429.546644 \t Validation Loss: 60570418.252155 \t Validation MAE: 2269.595393\n",
            "Epoch: 114 \t Training Loss: 2164037.161065 \t Training MAE: 474.880777 \t Validation Loss: 66860994.614224 \t Validation MAE: 2389.035772\n",
            "Epoch: 115 \t Training Loss: 2159445.386158 \t Training MAE: 551.542541 \t Validation Loss: 64291825.816810 \t Validation MAE: 2483.681572\n",
            "Epoch: 116 \t Training Loss: 2116007.467744 \t Training MAE: 438.460633 \t Validation Loss: 58517941.575431 \t Validation MAE: 2250.042818\n",
            "Epoch: 117 \t Training Loss: 1967656.875029 \t Training MAE: 411.619261 \t Validation Loss: 60872817.053879 \t Validation MAE: 2279.499729\n",
            "Epoch: 118 \t Training Loss: 1931510.367333 \t Training MAE: 415.623829 \t Validation Loss: 56945943.449353 \t Validation MAE: 2207.058537\n",
            "Epoch: 119 \t Training Loss: 2016620.062577 \t Training MAE: 419.544631 \t Validation Loss: 59084657.418103 \t Validation MAE: 2294.578591\n",
            "Epoch: 120 \t Training Loss: 2039184.477616 \t Training MAE: 474.266780 \t Validation Loss: 58831253.982759 \t Validation MAE: 2324.066125\n",
            "Epoch: 121 \t Training Loss: 2013175.292118 \t Training MAE: 425.593559 \t Validation Loss: 62790599.790948 \t Validation MAE: 2311.968293\n",
            "Epoch: 122 \t Training Loss: 1971112.346825 \t Training MAE: 446.319811 \t Validation Loss: 62161190.352371 \t Validation MAE: 2316.815447\n",
            "Epoch: 123 \t Training Loss: 1902690.467203 \t Training MAE: 423.707440 \t Validation Loss: 58828832.701509 \t Validation MAE: 2291.754472\n",
            "Epoch: 124 \t Training Loss: 1875558.655805 \t Training MAE: 421.154912 \t Validation Loss: 59703621.348060 \t Validation MAE: 2266.072629\n",
            "Epoch: 125 \t Training Loss: 1852753.956673 \t Training MAE: 423.537199 \t Validation Loss: 57683982.047414 \t Validation MAE: 2274.749051\n",
            "Epoch: 126 \t Training Loss: 1945158.193340 \t Training MAE: 439.683673 \t Validation Loss: 59110803.005388 \t Validation MAE: 2203.253117\n",
            "Epoch: 127 \t Training Loss: 1922524.624526 \t Training MAE: 431.631958 \t Validation Loss: 53400507.870690 \t Validation MAE: 2166.957182\n",
            "Epoch: 128 \t Training Loss: 1895675.792466 \t Training MAE: 421.458543 \t Validation Loss: 61452210.017241 \t Validation MAE: 2306.833062\n",
            "Epoch: 129 \t Training Loss: 1906082.932753 \t Training MAE: 429.731517 \t Validation Loss: 61057437.504310 \t Validation MAE: 2339.159079\n",
            "Epoch: 130 \t Training Loss: 1835598.967783 \t Training MAE: 412.029728 \t Validation Loss: 59214630.290948 \t Validation MAE: 2295.982114\n",
            "Epoch: 131 \t Training Loss: 1835434.505096 \t Training MAE: 397.473639 \t Validation Loss: 56670314.271552 \t Validation MAE: 2227.695664\n",
            "Epoch: 132 \t Training Loss: 1820772.367864 \t Training MAE: 407.725014 \t Validation Loss: 57401074.668642 \t Validation MAE: 2274.354743\n",
            "Epoch: 133 \t Training Loss: 1755553.488794 \t Training MAE: 428.146396 \t Validation Loss: 64195157.551724 \t Validation MAE: 2376.010840\n",
            "Epoch: 134 \t Training Loss: 1776971.973333 \t Training MAE: 418.672292 \t Validation Loss: 55069630.534483 \t Validation MAE: 2206.471816\n",
            "Epoch: 135 \t Training Loss: 1835867.377127 \t Training MAE: 426.910041 \t Validation Loss: 58332817.733836 \t Validation MAE: 2321.830081\n",
            "Epoch: 136 \t Training Loss: 1851530.599764 \t Training MAE: 433.669041 \t Validation Loss: 58910729.040948 \t Validation MAE: 2282.837940\n",
            "Epoch: 137 \t Training Loss: 1775242.283774 \t Training MAE: 405.441898 \t Validation Loss: 55667708.656250 \t Validation MAE: 2152.958266\n",
            "Epoch: 138 \t Training Loss: 1906778.489209 \t Training MAE: 417.573198 \t Validation Loss: 56049226.632543 \t Validation MAE: 2233.244444\n",
            "Epoch: 139 \t Training Loss: 1724601.298209 \t Training MAE: 391.667415 \t Validation Loss: 56037015.642241 \t Validation MAE: 2255.326016\n",
            "Epoch: 140 \t Training Loss: 1654359.417466 \t Training MAE: 419.186963 \t Validation Loss: 57616121.332974 \t Validation MAE: 2296.424932\n",
            "Epoch: 141 \t Training Loss: 1755656.705658 \t Training MAE: 393.272896 \t Validation Loss: 57267392.389009 \t Validation MAE: 2246.329268\n",
            "Epoch: 142 \t Training Loss: 1675326.486057 \t Training MAE: 393.524193 \t Validation Loss: 59138398.412177 \t Validation MAE: 2275.255014\n",
            "Epoch: 143 \t Training Loss: 1607271.337910 \t Training MAE: 390.084617 \t Validation Loss: 57303396.490841 \t Validation MAE: 2270.885095\n",
            "Epoch: 144 \t Training Loss: 1648108.398795 \t Training MAE: 410.715027 \t Validation Loss: 60097912.758621 \t Validation MAE: 2340.912737\n",
            "Epoch: 145 \t Training Loss: 1729557.076530 \t Training MAE: 426.152977 \t Validation Loss: 51804555.170259 \t Validation MAE: 2202.142005\n",
            "Epoch: 146 \t Training Loss: 1701167.332602 \t Training MAE: 410.141132 \t Validation Loss: 60346720.558728 \t Validation MAE: 2324.058808\n",
            "Epoch: 147 \t Training Loss: 1628698.153910 \t Training MAE: 397.902377 \t Validation Loss: 51112600.286638 \t Validation MAE: 2154.752846\n",
            "Epoch: 148 \t Training Loss: 1612570.246364 \t Training MAE: 397.829604 \t Validation Loss: 52630164.977909 \t Validation MAE: 2182.345799\n",
            "Epoch: 149 \t Training Loss: 1684891.756101 \t Training MAE: 389.263296 \t Validation Loss: 57038894.467672 \t Validation MAE: 2229.912466\n",
            "Epoch: 150 \t Training Loss: 1586768.880212 \t Training MAE: 374.381590 \t Validation Loss: 57391789.697198 \t Validation MAE: 2257.329539\n",
            "Epoch: 151 \t Training Loss: 1481099.685566 \t Training MAE: 378.665015 \t Validation Loss: 52952382.095905 \t Validation MAE: 2231.226287\n",
            "Epoch: 152 \t Training Loss: 1546229.895198 \t Training MAE: 404.721530 \t Validation Loss: 51576609.901401 \t Validation MAE: 2145.018157\n",
            "Epoch: 153 \t Training Loss: 1588918.168375 \t Training MAE: 394.461175 \t Validation Loss: 52538363.757543 \t Validation MAE: 2220.098103\n",
            "Epoch: 154 \t Training Loss: 1596119.340907 \t Training MAE: 406.179376 \t Validation Loss: 48453345.590517 \t Validation MAE: 2148.929539\n",
            "Epoch: 155 \t Training Loss: 1534465.130231 \t Training MAE: 410.132151 \t Validation Loss: 57221598.933190 \t Validation MAE: 2271.976965\n",
            "Epoch: 156 \t Training Loss: 1472368.011632 \t Training MAE: 387.968104 \t Validation Loss: 58502793.708513 \t Validation MAE: 2311.917344\n",
            "Epoch: 157 \t Training Loss: 1395038.272403 \t Training MAE: 378.574127 \t Validation Loss: 52698989.857220 \t Validation MAE: 2221.215447\n",
            "Epoch: 158 \t Training Loss: 1429345.027160 \t Training MAE: 443.383835 \t Validation Loss: 59292648.297414 \t Validation MAE: 2366.194309\n",
            "Epoch: 159 \t Training Loss: 1430750.515538 \t Training MAE: 395.030735 \t Validation Loss: 55857247.760237 \t Validation MAE: 2237.559892\n",
            "Epoch: 160 \t Training Loss: 1499280.725750 \t Training MAE: 403.147635 \t Validation Loss: 55442370.103448 \t Validation MAE: 2334.157453\n",
            "Epoch: 161 \t Training Loss: 1445960.494643 \t Training MAE: 398.227143 \t Validation Loss: 55014969.499461 \t Validation MAE: 2281.462602\n",
            "Epoch: 162 \t Training Loss: 1437972.485303 \t Training MAE: 390.704498 \t Validation Loss: 54133766.535022 \t Validation MAE: 2261.760705\n",
            "Epoch: 163 \t Training Loss: 1389063.192905 \t Training MAE: 374.127584 \t Validation Loss: 55035917.515625 \t Validation MAE: 2271.384824\n",
            "Epoch: 164 \t Training Loss: 1370581.058376 \t Training MAE: 398.330340 \t Validation Loss: 54891397.032866 \t Validation MAE: 2320.217886\n",
            "Epoch: 165 \t Training Loss: 1338985.528407 \t Training MAE: 375.289309 \t Validation Loss: 56303143.241918 \t Validation MAE: 2271.651220\n",
            "Epoch: 166 \t Training Loss: 1396713.975663 \t Training MAE: 399.712859 \t Validation Loss: 58956922.656250 \t Validation MAE: 2364.849864\n",
            "Epoch: 167 \t Training Loss: 1351602.699683 \t Training MAE: 365.915692 \t Validation Loss: 55357079.849138 \t Validation MAE: 2234.244444\n",
            "Epoch: 168 \t Training Loss: 1283348.145817 \t Training MAE: 372.637687 \t Validation Loss: 57643885.196121 \t Validation MAE: 2271.234959\n",
            "Epoch: 169 \t Training Loss: 1254244.053092 \t Training MAE: 374.933266 \t Validation Loss: 55626703.795259 \t Validation MAE: 2252.550678\n",
            "Epoch: 170 \t Training Loss: 1396767.251450 \t Training MAE: 396.509561 \t Validation Loss: 55850619.879310 \t Validation MAE: 2342.938211\n",
            "Epoch: 171 \t Training Loss: 1230919.131082 \t Training MAE: 360.838662 \t Validation Loss: 55125683.824353 \t Validation MAE: 2273.360976\n",
            "Epoch: 172 \t Training Loss: 1216140.698194 \t Training MAE: 370.163970 \t Validation Loss: 58645507.579203 \t Validation MAE: 2358.095664\n",
            "Epoch: 173 \t Training Loss: 1153256.726098 \t Training MAE: 370.231555 \t Validation Loss: 57833633.536099 \t Validation MAE: 2267.038211\n",
            "Epoch: 174 \t Training Loss: 1298325.101824 \t Training MAE: 420.257335 \t Validation Loss: 60272230.043103 \t Validation MAE: 2436.547967\n",
            "Epoch: 175 \t Training Loss: 1202956.735584 \t Training MAE: 383.379035 \t Validation Loss: 60280880.838362 \t Validation MAE: 2336.821680\n",
            "Epoch: 176 \t Training Loss: 1153677.908822 \t Training MAE: 480.816521 \t Validation Loss: 67331877.005388 \t Validation MAE: 2537.001897\n",
            "Epoch: 177 \t Training Loss: 1318852.175955 \t Training MAE: 430.790586 \t Validation Loss: 63654790.898707 \t Validation MAE: 2463.443631\n",
            "Epoch: 178 \t Training Loss: 1264870.791654 \t Training MAE: 388.570179 \t Validation Loss: 59886923.977371 \t Validation MAE: 2341.895664\n",
            "Epoch: 179 \t Training Loss: 1169293.284392 \t Training MAE: 381.775722 \t Validation Loss: 66870070.992457 \t Validation MAE: 2394.451762\n",
            "Epoch: 180 \t Training Loss: 1130156.145759 \t Training MAE: 357.825269 \t Validation Loss: 63455637.947198 \t Validation MAE: 2397.943902\n",
            "Epoch: 181 \t Training Loss: 1093061.541103 \t Training MAE: 385.235891 \t Validation Loss: 53071630.240302 \t Validation MAE: 2222.281301\n",
            "Epoch: 182 \t Training Loss: 1233260.225122 \t Training MAE: 394.539522 \t Validation Loss: 55655660.792026 \t Validation MAE: 2343.747425\n",
            "Epoch: 183 \t Training Loss: 1147457.980498 \t Training MAE: 347.093133 \t Validation Loss: 57647336.419720 \t Validation MAE: 2285.699458\n",
            "Epoch: 184 \t Training Loss: 1021975.028156 \t Training MAE: 372.305102 \t Validation Loss: 61357724.226293 \t Validation MAE: 2359.316260\n",
            "Epoch: 185 \t Training Loss: 1108758.619769 \t Training MAE: 355.123790 \t Validation Loss: 60315044.689116 \t Validation MAE: 2378.859350\n",
            "Epoch: 186 \t Training Loss: 1028605.332099 \t Training MAE: 360.596965 \t Validation Loss: 60157053.062500 \t Validation MAE: 2364.515176\n",
            "Epoch: 187 \t Training Loss: 1058790.761593 \t Training MAE: 347.755361 \t Validation Loss: 55110103.960129 \t Validation MAE: 2298.733333\n",
            "Epoch: 188 \t Training Loss: 1037094.279384 \t Training MAE: 354.754510 \t Validation Loss: 62335437.864224 \t Validation MAE: 2344.190515\n",
            "Epoch: 189 \t Training Loss: 1140547.239442 \t Training MAE: 391.606333 \t Validation Loss: 59723572.867457 \t Validation MAE: 2392.398374\n",
            "Epoch: 190 \t Training Loss: 1220511.260210 \t Training MAE: 339.501355 \t Validation Loss: 53741303.945043 \t Validation MAE: 2283.854743\n",
            "Epoch: 191 \t Training Loss: 1077950.848889 \t Training MAE: 379.230781 \t Validation Loss: 60872754.305496 \t Validation MAE: 2352.722493\n",
            "Epoch: 192 \t Training Loss: 1019754.353835 \t Training MAE: 358.492297 \t Validation Loss: 56929707.072198 \t Validation MAE: 2373.205962\n",
            "Epoch: 193 \t Training Loss: 1016229.460406 \t Training MAE: 353.632113 \t Validation Loss: 54411689.821121 \t Validation MAE: 2343.975339\n",
            "Epoch: 194 \t Training Loss: 983360.703782 \t Training MAE: 324.811334 \t Validation Loss: 57547897.500000 \t Validation MAE: 2314.539837\n",
            "Epoch: 195 \t Training Loss: 900569.615979 \t Training MAE: 346.702640 \t Validation Loss: 55492052.475216 \t Validation MAE: 2326.565583\n",
            "Epoch: 196 \t Training Loss: 982377.685179 \t Training MAE: 340.638461 \t Validation Loss: 52468460.744612 \t Validation MAE: 2261.988889\n",
            "Epoch: 197 \t Training Loss: 936167.315932 \t Training MAE: 334.365023 \t Validation Loss: 54397174.203125 \t Validation MAE: 2333.850136\n",
            "Epoch: 198 \t Training Loss: 975045.364587 \t Training MAE: 351.518387 \t Validation Loss: 55194455.664332 \t Validation MAE: 2330.261789\n",
            "Epoch: 199 \t Training Loss: 1011538.397925 \t Training MAE: 333.545405 \t Validation Loss: 53529339.541487 \t Validation MAE: 2285.770461\n",
            "Epoch: 200 \t Training Loss: 876413.533957 \t Training MAE: 327.487265 \t Validation Loss: 56548356.937500 \t Validation MAE: 2361.583198\n",
            "Epoch: 201 \t Training Loss: 999410.842039 \t Training MAE: 354.775644 \t Validation Loss: 54089456.148168 \t Validation MAE: 2270.815447\n",
            "Epoch: 202 \t Training Loss: 916573.786539 \t Training MAE: 356.065495 \t Validation Loss: 53861730.457974 \t Validation MAE: 2290.071003\n",
            "Epoch: 203 \t Training Loss: 862552.549515 \t Training MAE: 342.881629 \t Validation Loss: 58754675.894397 \t Validation MAE: 2393.365041\n",
            "Epoch: 204 \t Training Loss: 881838.723468 \t Training MAE: 335.760858 \t Validation Loss: 53352851.497845 \t Validation MAE: 2286.536043\n",
            "Epoch: 205 \t Training Loss: 1021626.235651 \t Training MAE: 346.306263 \t Validation Loss: 57732773.774784 \t Validation MAE: 2388.852575\n",
            "Epoch: 206 \t Training Loss: 877116.662989 \t Training MAE: 343.043896 \t Validation Loss: 60537425.098599 \t Validation MAE: 2413.060705\n",
            "Epoch: 207 \t Training Loss: 810784.059507 \t Training MAE: 341.042657 \t Validation Loss: 54085179.561961 \t Validation MAE: 2252.178591\n",
            "Epoch: 208 \t Training Loss: 895979.233650 \t Training MAE: 340.067276 \t Validation Loss: 57101208.240302 \t Validation MAE: 2323.184282\n",
            "Epoch: 209 \t Training Loss: 864626.584632 \t Training MAE: 332.622590 \t Validation Loss: 61004728.778017 \t Validation MAE: 2367.815447\n",
            "Epoch: 210 \t Training Loss: 861449.569510 \t Training MAE: 333.849191 \t Validation Loss: 55326211.680496 \t Validation MAE: 2265.022764\n",
            "Epoch: 211 \t Training Loss: 776550.947188 \t Training MAE: 391.801812 \t Validation Loss: 60569630.882543 \t Validation MAE: 2385.932791\n",
            "Epoch: 212 \t Training Loss: 877987.674553 \t Training MAE: 325.290470 \t Validation Loss: 56977557.803879 \t Validation MAE: 2348.641734\n",
            "Epoch: 213 \t Training Loss: 820329.746993 \t Training MAE: 344.972981 \t Validation Loss: 57229228.910022 \t Validation MAE: 2289.700542\n",
            "Epoch: 214 \t Training Loss: 802829.946869 \t Training MAE: 333.560424 \t Validation Loss: 60290495.984375 \t Validation MAE: 2331.485366\n",
            "Epoch: 215 \t Training Loss: 860824.137686 \t Training MAE: 330.066811 \t Validation Loss: 55529653.334052 \t Validation MAE: 2261.472900\n",
            "Epoch: 216 \t Training Loss: 880495.389354 \t Training MAE: 339.370984 \t Validation Loss: 59212089.328664 \t Validation MAE: 2318.709214\n",
            "Epoch: 217 \t Training Loss: 975936.200592 \t Training MAE: 334.479910 \t Validation Loss: 56894083.531789 \t Validation MAE: 2304.817886\n",
            "Epoch: 218 \t Training Loss: 1592731.211856 \t Training MAE: 332.475265 \t Validation Loss: 54166889.171875 \t Validation MAE: 2274.571274\n",
            "Epoch: 219 \t Training Loss: 754388.913690 \t Training MAE: 305.959046 \t Validation Loss: 56283236.496767 \t Validation MAE: 2334.362331\n",
            "Epoch: 220 \t Training Loss: 690356.626088 \t Training MAE: 314.137106 \t Validation Loss: 54292334.178341 \t Validation MAE: 2290.859079\n",
            "Epoch: 221 \t Training Loss: 719908.807907 \t Training MAE: 318.319811 \t Validation Loss: 56563168.073815 \t Validation MAE: 2272.674526\n",
            "Epoch: 222 \t Training Loss: 774974.382822 \t Training MAE: 326.205079 \t Validation Loss: 53285753.546875 \t Validation MAE: 2235.355014\n",
            "Epoch: 223 \t Training Loss: 754915.395015 \t Training MAE: 327.353565 \t Validation Loss: 54846725.768319 \t Validation MAE: 2267.831978\n",
            "Epoch: 224 \t Training Loss: 848060.722690 \t Training MAE: 332.388867 \t Validation Loss: 59150245.131466 \t Validation MAE: 2334.005962\n",
            "Epoch: 225 \t Training Loss: 772248.489693 \t Training MAE: 310.060076 \t Validation Loss: 56053820.193966 \t Validation MAE: 2306.195122\n",
            "Epoch: 226 \t Training Loss: 733487.752219 \t Training MAE: 319.150190 \t Validation Loss: 56746924.276401 \t Validation MAE: 2376.337127\n",
            "Epoch: 227 \t Training Loss: 700775.784629 \t Training MAE: 316.283425 \t Validation Loss: 57541066.188578 \t Validation MAE: 2367.078591\n",
            "Epoch: 228 \t Training Loss: 709821.189004 \t Training MAE: 309.885035 \t Validation Loss: 56009497.265086 \t Validation MAE: 2329.578862\n",
            "Epoch: 229 \t Training Loss: 690135.335488 \t Training MAE: 337.172176 \t Validation Loss: 66533617.975216 \t Validation MAE: 2458.972358\n",
            "Epoch: 230 \t Training Loss: 783607.509891 \t Training MAE: 365.405667 \t Validation Loss: 61078015.875000 \t Validation MAE: 2450.950678\n",
            "Epoch: 231 \t Training Loss: 732485.910422 \t Training MAE: 312.803128 \t Validation Loss: 53941851.827586 \t Validation MAE: 2245.411924\n",
            "Epoch: 232 \t Training Loss: 681015.970316 \t Training MAE: 318.282573 \t Validation Loss: 58451626.046336 \t Validation MAE: 2371.268564\n",
            "Epoch: 233 \t Training Loss: 672862.106948 \t Training MAE: 310.409306 \t Validation Loss: 55131892.660560 \t Validation MAE: 2299.420596\n",
            "Epoch: 234 \t Training Loss: 685348.104961 \t Training MAE: 303.152822 \t Validation Loss: 53984111.764009 \t Validation MAE: 2261.597019\n",
            "Epoch: 235 \t Training Loss: 749135.874091 \t Training MAE: 320.210343 \t Validation Loss: 57371743.095905 \t Validation MAE: 2268.540379\n",
            "Epoch: 236 \t Training Loss: 748213.512555 \t Training MAE: 318.221491 \t Validation Loss: 55642765.041487 \t Validation MAE: 2334.936043\n",
            "Epoch: 237 \t Training Loss: 747402.551429 \t Training MAE: 317.842301 \t Validation Loss: 58779417.660560 \t Validation MAE: 2359.156369\n",
            "Epoch: 238 \t Training Loss: 659621.221699 \t Training MAE: 379.885422 \t Validation Loss: 62800633.631466 \t Validation MAE: 2514.109214\n",
            "Epoch: 239 \t Training Loss: 716790.591434 \t Training MAE: 299.035689 \t Validation Loss: 55092946.772091 \t Validation MAE: 2310.060976\n",
            "Epoch: 240 \t Training Loss: 768058.137231 \t Training MAE: 313.223117 \t Validation Loss: 61054854.890625 \t Validation MAE: 2326.182385\n",
            "Epoch: 241 \t Training Loss: 675451.561417 \t Training MAE: 312.504761 \t Validation Loss: 59761064.451509 \t Validation MAE: 2383.588618\n",
            "Epoch: 242 \t Training Loss: 620223.012323 \t Training MAE: 295.888209 \t Validation Loss: 56523152.324353 \t Validation MAE: 2303.107046\n",
            "Epoch: 243 \t Training Loss: 608867.210841 \t Training MAE: 296.629635 \t Validation Loss: 60081823.063578 \t Validation MAE: 2392.935230\n",
            "Epoch: 244 \t Training Loss: 659522.803053 \t Training MAE: 328.213285 \t Validation Loss: 57887563.570043 \t Validation MAE: 2402.485366\n",
            "Epoch: 245 \t Training Loss: 682194.425341 \t Training MAE: 314.895719 \t Validation Loss: 59500171.249461 \t Validation MAE: 2360.537669\n",
            "Epoch: 246 \t Training Loss: 684664.819007 \t Training MAE: 300.455756 \t Validation Loss: 55754484.441810 \t Validation MAE: 2254.680759\n",
            "Epoch: 247 \t Training Loss: 647830.890122 \t Training MAE: 310.538980 \t Validation Loss: 55679871.433190 \t Validation MAE: 2243.164770\n",
            "Epoch: 248 \t Training Loss: 637915.480768 \t Training MAE: 326.095610 \t Validation Loss: 55849359.087823 \t Validation MAE: 2272.800813\n",
            "Epoch: 249 \t Training Loss: 693923.075485 \t Training MAE: 306.195943 \t Validation Loss: 55114273.487069 \t Validation MAE: 2243.091057\n",
            "Epoch: 250 \t Training Loss: 734717.972192 \t Training MAE: 337.929550 \t Validation Loss: 63002715.868534 \t Validation MAE: 2451.525474\n",
            "Epoch: 251 \t Training Loss: 671003.200988 \t Training MAE: 324.612991 \t Validation Loss: 54418977.178071 \t Validation MAE: 2226.674797\n",
            "Epoch: 252 \t Training Loss: 587454.415290 \t Training MAE: 281.359217 \t Validation Loss: 55548013.341595 \t Validation MAE: 2300.319512\n",
            "Epoch: 253 \t Training Loss: 630422.096583 \t Training MAE: 289.159325 \t Validation Loss: 58399280.108297 \t Validation MAE: 2279.086721\n",
            "Epoch: 254 \t Training Loss: 655226.313815 \t Training MAE: 311.791128 \t Validation Loss: 55379211.060345 \t Validation MAE: 2328.269648\n",
            "Epoch: 255 \t Training Loss: 639457.802246 \t Training MAE: 285.154835 \t Validation Loss: 58650861.261315 \t Validation MAE: 2318.882927\n",
            "Epoch: 256 \t Training Loss: 673430.078594 \t Training MAE: 305.896183 \t Validation Loss: 56168081.188578 \t Validation MAE: 2310.105149\n",
            "Epoch: 257 \t Training Loss: 656056.980570 \t Training MAE: 289.554618 \t Validation Loss: 56720527.924569 \t Validation MAE: 2302.105420\n",
            "Epoch: 258 \t Training Loss: 692149.352863 \t Training MAE: 315.830611 \t Validation Loss: 60183153.712284 \t Validation MAE: 2383.596477\n",
            "Epoch: 259 \t Training Loss: 670082.564743 \t Training MAE: 304.688937 \t Validation Loss: 60899207.745151 \t Validation MAE: 2386.432249\n",
            "Epoch: 260 \t Training Loss: 637986.264503 \t Training MAE: 303.634358 \t Validation Loss: 55806579.168103 \t Validation MAE: 2273.127371\n",
            "Epoch: 261 \t Training Loss: 603191.100199 \t Training MAE: 278.462414 \t Validation Loss: 53826936.074353 \t Validation MAE: 2296.491870\n",
            "Epoch: 262 \t Training Loss: 586571.400676 \t Training MAE: 274.201285 \t Validation Loss: 57265225.964709 \t Validation MAE: 2299.407859\n",
            "Epoch: 263 \t Training Loss: 574411.621679 \t Training MAE: 277.950143 \t Validation Loss: 53852484.741379 \t Validation MAE: 2293.228726\n",
            "Epoch: 264 \t Training Loss: 635949.007078 \t Training MAE: 304.743749 \t Validation Loss: 59258548.517241 \t Validation MAE: 2334.271274\n",
            "Epoch: 265 \t Training Loss: 701694.855333 \t Training MAE: 304.263838 \t Validation Loss: 54732576.172414 \t Validation MAE: 2269.665312\n",
            "Epoch: 266 \t Training Loss: 667970.746287 \t Training MAE: 291.229697 \t Validation Loss: 62261569.046875 \t Validation MAE: 2318.129268\n",
            "Epoch: 267 \t Training Loss: 622975.109747 \t Training MAE: 296.179841 \t Validation Loss: 58064461.565194 \t Validation MAE: 2363.530352\n",
            "Epoch: 268 \t Training Loss: 609102.718532 \t Training MAE: 276.297360 \t Validation Loss: 56811515.596444 \t Validation MAE: 2293.150136\n",
            "Epoch: 269 \t Training Loss: 562171.722400 \t Training MAE: 273.715723 \t Validation Loss: 55870742.197198 \t Validation MAE: 2304.941192\n",
            "Epoch: 270 \t Training Loss: 572776.610352 \t Training MAE: 274.845088 \t Validation Loss: 54355400.663793 \t Validation MAE: 2311.392141\n",
            "Epoch: 271 \t Training Loss: 594866.995093 \t Training MAE: 283.350236 \t Validation Loss: 56114744.290409 \t Validation MAE: 2301.771545\n",
            "Epoch: 272 \t Training Loss: 731440.884205 \t Training MAE: 345.618565 \t Validation Loss: 51229165.018858 \t Validation MAE: 2300.013550\n",
            "Epoch: 273 \t Training Loss: 635939.458192 \t Training MAE: 294.768832 \t Validation Loss: 59287542.046336 \t Validation MAE: 2397.817344\n",
            "Epoch: 274 \t Training Loss: 602261.827443 \t Training MAE: 291.319269 \t Validation Loss: 54130556.962284 \t Validation MAE: 2324.986179\n",
            "Epoch: 275 \t Training Loss: 571297.267588 \t Training MAE: 272.350391 \t Validation Loss: 56794905.415409 \t Validation MAE: 2352.160434\n",
            "Epoch: 276 \t Training Loss: 564006.832041 \t Training MAE: 291.777115 \t Validation Loss: 55862753.145744 \t Validation MAE: 2362.534146\n",
            "Epoch: 277 \t Training Loss: 592585.098700 \t Training MAE: 281.729117 \t Validation Loss: 52936120.596444 \t Validation MAE: 2303.191328\n",
            "Epoch: 278 \t Training Loss: 685503.468765 \t Training MAE: 290.100797 \t Validation Loss: 61090608.674569 \t Validation MAE: 2404.580488\n",
            "Epoch: 279 \t Training Loss: 650424.496539 \t Training MAE: 295.268793 \t Validation Loss: 54309395.092672 \t Validation MAE: 2290.135501\n",
            "Epoch: 280 \t Training Loss: 589343.991598 \t Training MAE: 271.380506 \t Validation Loss: 53970627.129041 \t Validation MAE: 2319.860705\n",
            "Epoch: 281 \t Training Loss: 592137.548785 \t Training MAE: 280.755516 \t Validation Loss: 56283307.211746 \t Validation MAE: 2378.798103\n",
            "Epoch: 282 \t Training Loss: 568002.658053 \t Training MAE: 274.344430 \t Validation Loss: 55772950.160291 \t Validation MAE: 2312.668835\n",
            "Epoch: 283 \t Training Loss: 575920.371103 \t Training MAE: 272.919331 \t Validation Loss: 58749055.056573 \t Validation MAE: 2356.362060\n",
            "Epoch: 284 \t Training Loss: 596318.001349 \t Training MAE: 278.084772 \t Validation Loss: 58111233.111530 \t Validation MAE: 2329.098374\n",
            "Epoch: 285 \t Training Loss: 575292.304789 \t Training MAE: 277.145390 \t Validation Loss: 56274993.665948 \t Validation MAE: 2355.814363\n",
            "Epoch: 286 \t Training Loss: 636717.604855 \t Training MAE: 289.306031 \t Validation Loss: 56276138.635237 \t Validation MAE: 2350.285095\n",
            "Epoch: 287 \t Training Loss: 608453.318823 \t Training MAE: 304.304095 \t Validation Loss: 57811071.977371 \t Validation MAE: 2425.007317\n",
            "Epoch: 288 \t Training Loss: 714670.736879 \t Training MAE: 292.355810 \t Validation Loss: 58601412.175647 \t Validation MAE: 2420.135501\n",
            "Epoch: 289 \t Training Loss: 593392.243077 \t Training MAE: 272.619184 \t Validation Loss: 58972469.385237 \t Validation MAE: 2384.468835\n",
            "Epoch: 290 \t Training Loss: 595770.977150 \t Training MAE: 285.470775 \t Validation Loss: 54741020.055765 \t Validation MAE: 2291.657724\n",
            "Epoch: 291 \t Training Loss: 564273.890775 \t Training MAE: 277.176976 \t Validation Loss: 59645952.704741 \t Validation MAE: 2433.371003\n",
            "Epoch: 292 \t Training Loss: 563200.579106 \t Training MAE: 320.505845 \t Validation Loss: 61184191.865841 \t Validation MAE: 2476.699187\n",
            "Epoch: 293 \t Training Loss: 653121.021649 \t Training MAE: 290.802586 \t Validation Loss: 56167101.740302 \t Validation MAE: 2384.461789\n",
            "Epoch: 294 \t Training Loss: 613955.789270 \t Training MAE: 314.103894 \t Validation Loss: 61726015.659483 \t Validation MAE: 2456.931707\n",
            "Epoch: 295 \t Training Loss: 663223.347081 \t Training MAE: 273.963614 \t Validation Loss: 54788740.438578 \t Validation MAE: 2360.274797\n",
            "Epoch: 296 \t Training Loss: 579856.473618 \t Training MAE: 276.461330 \t Validation Loss: 56494066.462015 \t Validation MAE: 2327.082385\n",
            "Epoch: 297 \t Training Loss: 582926.302319 \t Training MAE: 258.064102 \t Validation Loss: 58442891.695043 \t Validation MAE: 2393.265041\n",
            "Epoch: 298 \t Training Loss: 526740.000314 \t Training MAE: 263.599597 \t Validation Loss: 55258594.924569 \t Validation MAE: 2377.017886\n",
            "Epoch: 299 \t Training Loss: 620403.532130 \t Training MAE: 279.572347 \t Validation Loss: 60107513.870151 \t Validation MAE: 2412.250949\n",
            "Epoch: 300 \t Training Loss: 549663.131937 \t Training MAE: 271.723388 \t Validation Loss: 55573288.561961 \t Validation MAE: 2339.454201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU1JuOCGcIvN"
      },
      "source": [
        "### 6. **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjQhOCG2Zk1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21daf26f-7e00-4124-f9ae-000bcf0b466c"
      },
      "source": [
        "_, test_mae, _ = evaluate(model, test_loader)\n",
        "print('Test MAE: %.2f'%(test_mae))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test MAE: 2133.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piuf5sBXg7A-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}